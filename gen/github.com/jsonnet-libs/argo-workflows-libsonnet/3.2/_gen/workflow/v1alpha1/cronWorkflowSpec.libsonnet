{
  local d = (import 'doc-util/main.libsonnet'),
  '#':: d.pkg(name='cronWorkflowSpec', url='', help='"CronWorkflowSpec is the specification of a CronWorkflow"'),
  '#withConcurrencyPolicy':: d.fn(help='"ConcurrencyPolicy is the K8s-style concurrency policy that will be used"', args=[d.arg(name='concurrencyPolicy', type=d.T.string)]),
  withConcurrencyPolicy(concurrencyPolicy): { concurrencyPolicy: concurrencyPolicy },
  '#withFailedJobsHistoryLimit':: d.fn(help='"FailedJobsHistoryLimit is the number of failed jobs to be kept at a time"', args=[d.arg(name='failedJobsHistoryLimit', type=d.T.integer)]),
  withFailedJobsHistoryLimit(failedJobsHistoryLimit): { failedJobsHistoryLimit: failedJobsHistoryLimit },
  '#withSchedule':: d.fn(help='"Schedule is a schedule to run the Workflow in Cron format"', args=[d.arg(name='schedule', type=d.T.string)]),
  withSchedule(schedule): { schedule: schedule },
  '#withStartingDeadlineSeconds':: d.fn(help='"StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run after its original scheduled time if it is missed."', args=[d.arg(name='startingDeadlineSeconds', type=d.T.integer)]),
  withStartingDeadlineSeconds(startingDeadlineSeconds): { startingDeadlineSeconds: startingDeadlineSeconds },
  '#withSuccessfulJobsHistoryLimit':: d.fn(help='"SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time"', args=[d.arg(name='successfulJobsHistoryLimit', type=d.T.integer)]),
  withSuccessfulJobsHistoryLimit(successfulJobsHistoryLimit): { successfulJobsHistoryLimit: successfulJobsHistoryLimit },
  '#withSuspend':: d.fn(help='"Suspend is a flag that will stop new CronWorkflows from running if set to true"', args=[d.arg(name='suspend', type=d.T.boolean)]),
  withSuspend(suspend): { suspend: suspend },
  '#withTimezone':: d.fn(help="\"Timezone is the timezone against which the cron schedule will be calculated, e.g. \\\"Asia/Tokyo\\\". Default is machine's local time.\"", args=[d.arg(name='timezone', type=d.T.string)]),
  withTimezone(timezone): { timezone: timezone },
  '#workflowMetadata':: d.obj(help='"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create."'),
  workflowMetadata: {
    '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotations(annotations): { workflowMetadata+: { annotations: annotations } },
    '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotationsMixin(annotations): { workflowMetadata+: { annotations+: annotations } },
    '#withClusterName':: d.fn(help='"The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request."', args=[d.arg(name='clusterName', type=d.T.string)]),
    withClusterName(clusterName): { workflowMetadata+: { clusterName: clusterName } },
    '#withCreationTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='creationTimestamp', type=d.T.string)]),
    withCreationTimestamp(creationTimestamp): { workflowMetadata+: { creationTimestamp: creationTimestamp } },
    '#withDeletionGracePeriodSeconds':: d.fn(help='"Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."', args=[d.arg(name='deletionGracePeriodSeconds', type=d.T.integer)]),
    withDeletionGracePeriodSeconds(deletionGracePeriodSeconds): { workflowMetadata+: { deletionGracePeriodSeconds: deletionGracePeriodSeconds } },
    '#withDeletionTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='deletionTimestamp', type=d.T.string)]),
    withDeletionTimestamp(deletionTimestamp): { workflowMetadata+: { deletionTimestamp: deletionTimestamp } },
    '#withFinalizers':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizers(finalizers): { workflowMetadata+: { finalizers: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withFinalizersMixin':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizersMixin(finalizers): { workflowMetadata+: { finalizers+: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withGenerateName':: d.fn(help='"GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.\\n\\nIf this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).\\n\\nApplied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"', args=[d.arg(name='generateName', type=d.T.string)]),
    withGenerateName(generateName): { workflowMetadata+: { generateName: generateName } },
    '#withGeneration':: d.fn(help='"A sequence number representing a specific generation of the desired state. Populated by the system. Read-only."', args=[d.arg(name='generation', type=d.T.integer)]),
    withGeneration(generation): { workflowMetadata+: { generation: generation } },
    '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
    withLabels(labels): { workflowMetadata+: { labels: labels } },
    '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
    withLabelsMixin(labels): { workflowMetadata+: { labels+: labels } },
    '#withManagedFields':: d.fn(help="\"ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \\\"ci-cd\\\". The set of fields is always in the version that the workflow used when modifying the object.\"", args=[d.arg(name='managedFields', type=d.T.array)]),
    withManagedFields(managedFields): { workflowMetadata+: { managedFields: if std.isArray(v=managedFields) then managedFields else [managedFields] } },
    '#withManagedFieldsMixin':: d.fn(help="\"ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \\\"ci-cd\\\". The set of fields is always in the version that the workflow used when modifying the object.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='managedFields', type=d.T.array)]),
    withManagedFieldsMixin(managedFields): { workflowMetadata+: { managedFields+: if std.isArray(v=managedFields) then managedFields else [managedFields] } },
    '#withName':: d.fn(help='"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names"', args=[d.arg(name='name', type=d.T.string)]),
    withName(name): { workflowMetadata+: { name: name } },
    '#withNamespace':: d.fn(help='"Namespace defines the space within each name must be unique. An empty namespace is equivalent to the \\"default\\" namespace, but \\"default\\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.\\n\\nMust be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"', args=[d.arg(name='namespace', type=d.T.string)]),
    withNamespace(namespace): { workflowMetadata+: { namespace: namespace } },
    '#withOwnerReferences':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferences(ownerReferences): { workflowMetadata+: { ownerReferences: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withOwnerReferencesMixin':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferencesMixin(ownerReferences): { workflowMetadata+: { ownerReferences+: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withResourceVersion':: d.fn(help='"An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.\\n\\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"', args=[d.arg(name='resourceVersion', type=d.T.string)]),
    withResourceVersion(resourceVersion): { workflowMetadata+: { resourceVersion: resourceVersion } },
    '#withSelfLink':: d.fn(help='"SelfLink is a URL representing this object. Populated by the system. Read-only.\\n\\nDEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."', args=[d.arg(name='selfLink', type=d.T.string)]),
    withSelfLink(selfLink): { workflowMetadata+: { selfLink: selfLink } },
    '#withUid':: d.fn(help='"UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.\\n\\nPopulated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"', args=[d.arg(name='uid', type=d.T.string)]),
    withUid(uid): { workflowMetadata+: { uid: uid } },
  },
  '#workflowSpec':: d.obj(help='"WorkflowSpec is the specification of a Workflow."'),
  workflowSpec: {
    '#affinity':: d.obj(help='"Affinity is a group of affinity scheduling rules."'),
    affinity: {
      '#nodeAffinity':: d.obj(help='"Node affinity is a group of node affinity scheduling rules."'),
      nodeAffinity: {
        '#requiredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms."'),
        requiredDuringSchedulingIgnoredDuringExecution: {
          '#withNodeSelectorTerms':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
          withNodeSelectorTerms(nodeSelectorTerms): { workflowSpec+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } },
          '#withNodeSelectorTermsMixin':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
          withNodeSelectorTermsMixin(nodeSelectorTerms): { workflowSpec+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms+: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } },
        },
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
      },
      '#podAffinity':: d.obj(help='"Pod affinity is a group of inter pod affinity scheduling rules."'),
      podAffinity: {
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
      },
      '#podAntiAffinity':: d.obj(help='"Pod anti affinity is a group of inter pod anti affinity scheduling rules."'),
      podAntiAffinity: {
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { affinity+: { podAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
      },
    },
    '#arguments':: d.obj(help='"Arguments to a template"'),
    arguments: {
      '#withArtifacts':: d.fn(help='"Artifacts is the list of artifacts to pass to the template or workflow"', args=[d.arg(name='artifacts', type=d.T.array)]),
      withArtifacts(artifacts): { workflowSpec+: { arguments+: { artifacts: if std.isArray(v=artifacts) then artifacts else [artifacts] } } },
      '#withArtifactsMixin':: d.fn(help='"Artifacts is the list of artifacts to pass to the template or workflow"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='artifacts', type=d.T.array)]),
      withArtifactsMixin(artifacts): { workflowSpec+: { arguments+: { artifacts+: if std.isArray(v=artifacts) then artifacts else [artifacts] } } },
      '#withParameters':: d.fn(help='"Parameters is the list of parameters to pass to the template or workflow"', args=[d.arg(name='parameters', type=d.T.array)]),
      withParameters(parameters): { workflowSpec+: { arguments+: { parameters: if std.isArray(v=parameters) then parameters else [parameters] } } },
      '#withParametersMixin':: d.fn(help='"Parameters is the list of parameters to pass to the template or workflow"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
      withParametersMixin(parameters): { workflowSpec+: { arguments+: { parameters+: if std.isArray(v=parameters) then parameters else [parameters] } } },
    },
    '#artifactRepositoryRef':: d.obj(help=''),
    artifactRepositoryRef: {
      '#withConfigMap':: d.fn(help='"The name of the config map. Defaults to \\"artifact-repositories\\"."', args=[d.arg(name='configMap', type=d.T.string)]),
      withConfigMap(configMap): { workflowSpec+: { artifactRepositoryRef+: { configMap: configMap } } },
      '#withKey':: d.fn(help='"The config map key. Defaults to the value of the \\"workflows.argoproj.io/default-artifact-repository\\" annotation."', args=[d.arg(name='key', type=d.T.string)]),
      withKey(key): { workflowSpec+: { artifactRepositoryRef+: { key: key } } },
    },
    '#dnsConfig':: d.obj(help='"PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy."'),
    dnsConfig: {
      '#withNameservers':: d.fn(help='"A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed."', args=[d.arg(name='nameservers', type=d.T.array)]),
      withNameservers(nameservers): { workflowSpec+: { dnsConfig+: { nameservers: if std.isArray(v=nameservers) then nameservers else [nameservers] } } },
      '#withNameserversMixin':: d.fn(help='"A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nameservers', type=d.T.array)]),
      withNameserversMixin(nameservers): { workflowSpec+: { dnsConfig+: { nameservers+: if std.isArray(v=nameservers) then nameservers else [nameservers] } } },
      '#withOptions':: d.fn(help='"A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy."', args=[d.arg(name='options', type=d.T.array)]),
      withOptions(options): { workflowSpec+: { dnsConfig+: { options: if std.isArray(v=options) then options else [options] } } },
      '#withOptionsMixin':: d.fn(help='"A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='options', type=d.T.array)]),
      withOptionsMixin(options): { workflowSpec+: { dnsConfig+: { options+: if std.isArray(v=options) then options else [options] } } },
      '#withSearches':: d.fn(help='"A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed."', args=[d.arg(name='searches', type=d.T.array)]),
      withSearches(searches): { workflowSpec+: { dnsConfig+: { searches: if std.isArray(v=searches) then searches else [searches] } } },
      '#withSearchesMixin':: d.fn(help='"A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='searches', type=d.T.array)]),
      withSearchesMixin(searches): { workflowSpec+: { dnsConfig+: { searches+: if std.isArray(v=searches) then searches else [searches] } } },
    },
    '#executor':: d.obj(help='"ExecutorConfig holds configurations of an executor container."'),
    executor: {
      '#withServiceAccountName':: d.fn(help='"ServiceAccountName specifies the service account name of the executor container."', args=[d.arg(name='serviceAccountName', type=d.T.string)]),
      withServiceAccountName(serviceAccountName): { workflowSpec+: { executor+: { serviceAccountName: serviceAccountName } } },
    },
    '#metrics':: d.obj(help='"Metrics are a list of metrics emitted from a Workflow/Template"'),
    metrics: {
      '#withPrometheus':: d.fn(help='"Prometheus is a list of prometheus metrics to be emitted"', args=[d.arg(name='prometheus', type=d.T.array)]),
      withPrometheus(prometheus): { workflowSpec+: { metrics+: { prometheus: if std.isArray(v=prometheus) then prometheus else [prometheus] } } },
      '#withPrometheusMixin':: d.fn(help='"Prometheus is a list of prometheus metrics to be emitted"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='prometheus', type=d.T.array)]),
      withPrometheusMixin(prometheus): { workflowSpec+: { metrics+: { prometheus+: if std.isArray(v=prometheus) then prometheus else [prometheus] } } },
    },
    '#podDisruptionBudget':: d.obj(help='"PodDisruptionBudgetSpec is a description of a PodDisruptionBudget."'),
    podDisruptionBudget: {
      '#selector':: d.obj(help='"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects."'),
      selector: {
        '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressions(matchExpressions): { workflowSpec+: { podDisruptionBudget+: { selector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
        '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressionsMixin(matchExpressions): { workflowSpec+: { podDisruptionBudget+: { selector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
        '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\"key\\", the operator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { workflowSpec+: { podDisruptionBudget+: { selector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\"key\\", the operator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { workflowSpec+: { podDisruptionBudget+: { selector+: { matchLabels+: matchLabels } } } },
      },
      '#withMaxUnavailable':: d.fn(help='', args=[d.arg(name='maxUnavailable', type=d.T.string)]),
      withMaxUnavailable(maxUnavailable): { workflowSpec+: { podDisruptionBudget+: { maxUnavailable: maxUnavailable } } },
      '#withMinAvailable':: d.fn(help='', args=[d.arg(name='minAvailable', type=d.T.string)]),
      withMinAvailable(minAvailable): { workflowSpec+: { podDisruptionBudget+: { minAvailable: minAvailable } } },
    },
    '#podGC':: d.obj(help='"PodGC describes how to delete completed pods as they complete"'),
    podGC: {
      '#labelSelector':: d.obj(help='"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects."'),
      labelSelector: {
        '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressions(matchExpressions): { workflowSpec+: { podGC+: { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
        '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressionsMixin(matchExpressions): { workflowSpec+: { podGC+: { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
        '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\"key\\", the operator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { workflowSpec+: { podGC+: { labelSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\"key\\", the operator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { workflowSpec+: { podGC+: { labelSelector+: { matchLabels+: matchLabels } } } },
      },
      '#withStrategy':: d.fn(help='"Strategy is the strategy to use. One of \\"OnPodCompletion\\", \\"OnPodSuccess\\", \\"OnWorkflowCompletion\\", \\"OnWorkflowSuccess\\', args=[d.arg(name='strategy', type=d.T.string)]),
      withStrategy(strategy): { workflowSpec+: { podGC+: { strategy: strategy } } },
    },
    '#podMetadata':: d.obj(help='"Pod metdata"'),
    podMetadata: {
      '#withAnnotations':: d.fn(help='', args=[d.arg(name='annotations', type=d.T.object)]),
      withAnnotations(annotations): { workflowSpec+: { podMetadata+: { annotations: annotations } } },
      '#withAnnotationsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
      withAnnotationsMixin(annotations): { workflowSpec+: { podMetadata+: { annotations+: annotations } } },
      '#withLabels':: d.fn(help='', args=[d.arg(name='labels', type=d.T.object)]),
      withLabels(labels): { workflowSpec+: { podMetadata+: { labels: labels } } },
      '#withLabelsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
      withLabelsMixin(labels): { workflowSpec+: { podMetadata+: { labels+: labels } } },
    },
    '#retryStrategy':: d.obj(help='"RetryStrategy provides controls on how to retry a workflow step"'),
    retryStrategy: {
      '#affinity':: d.obj(help='"RetryAffinity prevents running steps on the same host."'),
      affinity: {
        '#withNodeAntiAffinity':: d.fn(help='"RetryNodeAntiAffinity is a placeholder for future expansion, only empty nodeAntiAffinity is allowed. In order to prevent running steps on the same host, it uses \\"kubernetes.io/hostname\\"."', args=[d.arg(name='nodeAntiAffinity', type=d.T.object)]),
        withNodeAntiAffinity(nodeAntiAffinity): { workflowSpec+: { retryStrategy+: { affinity+: { nodeAntiAffinity: nodeAntiAffinity } } } },
        '#withNodeAntiAffinityMixin':: d.fn(help='"RetryNodeAntiAffinity is a placeholder for future expansion, only empty nodeAntiAffinity is allowed. In order to prevent running steps on the same host, it uses \\"kubernetes.io/hostname\\"."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeAntiAffinity', type=d.T.object)]),
        withNodeAntiAffinityMixin(nodeAntiAffinity): { workflowSpec+: { retryStrategy+: { affinity+: { nodeAntiAffinity+: nodeAntiAffinity } } } },
      },
      '#backoff':: d.obj(help='"Backoff is a backoff strategy to use within retryStrategy"'),
      backoff: {
        '#withDuration':: d.fn(help='"Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \\"2m\\", \\"1h\\")"', args=[d.arg(name='duration', type=d.T.string)]),
        withDuration(duration): { workflowSpec+: { retryStrategy+: { backoff+: { duration: duration } } } },
        '#withFactor':: d.fn(help='', args=[d.arg(name='factor', type=d.T.string)]),
        withFactor(factor): { workflowSpec+: { retryStrategy+: { backoff+: { factor: factor } } } },
        '#withMaxDuration':: d.fn(help='"MaxDuration is the maximum amount of time allowed for the backoff strategy"', args=[d.arg(name='maxDuration', type=d.T.string)]),
        withMaxDuration(maxDuration): { workflowSpec+: { retryStrategy+: { backoff+: { maxDuration: maxDuration } } } },
      },
      '#withExpression':: d.fn(help='"Expression is a condition expression for when a node will be retried. If it evaluates to false, the node will not be retried and the retry strategy will be ignored/"', args=[d.arg(name='expression', type=d.T.string)]),
      withExpression(expression): { workflowSpec+: { retryStrategy+: { expression: expression } } },
      '#withLimit':: d.fn(help='', args=[d.arg(name='limit', type=d.T.string)]),
      withLimit(limit): { workflowSpec+: { retryStrategy+: { limit: limit } } },
      '#withRetryPolicy':: d.fn(help='"RetryPolicy is a policy of NodePhase statuses that will be retried"', args=[d.arg(name='retryPolicy', type=d.T.string)]),
      withRetryPolicy(retryPolicy): { workflowSpec+: { retryStrategy+: { retryPolicy: retryPolicy } } },
    },
    '#securityContext':: d.obj(help='"PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext."'),
    securityContext: {
      '#seLinuxOptions':: d.obj(help='"SELinuxOptions are the labels to be applied to the container"'),
      seLinuxOptions: {
        '#withLevel':: d.fn(help='"Level is SELinux level label that applies to the container."', args=[d.arg(name='level', type=d.T.string)]),
        withLevel(level): { workflowSpec+: { securityContext+: { seLinuxOptions+: { level: level } } } },
        '#withRole':: d.fn(help='"Role is a SELinux role label that applies to the container."', args=[d.arg(name='role', type=d.T.string)]),
        withRole(role): { workflowSpec+: { securityContext+: { seLinuxOptions+: { role: role } } } },
        '#withType':: d.fn(help='"Type is a SELinux type label that applies to the container."', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { workflowSpec+: { securityContext+: { seLinuxOptions+: { type: type } } } },
        '#withUser':: d.fn(help='"User is a SELinux user label that applies to the container."', args=[d.arg(name='user', type=d.T.string)]),
        withUser(user): { workflowSpec+: { securityContext+: { seLinuxOptions+: { user: user } } } },
      },
      '#windowsOptions':: d.obj(help='"WindowsSecurityContextOptions contain Windows-specific options and credentials."'),
      windowsOptions: {
        '#withGmsaCredentialSpec':: d.fn(help='"GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpec', type=d.T.string)]),
        withGmsaCredentialSpec(gmsaCredentialSpec): { workflowSpec+: { securityContext+: { windowsOptions+: { gmsaCredentialSpec: gmsaCredentialSpec } } } },
        '#withGmsaCredentialSpecName':: d.fn(help='"GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpecName', type=d.T.string)]),
        withGmsaCredentialSpecName(gmsaCredentialSpecName): { workflowSpec+: { securityContext+: { windowsOptions+: { gmsaCredentialSpecName: gmsaCredentialSpecName } } } },
        '#withRunAsUserName':: d.fn(help='"The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. This field is beta-level and may be disabled with the WindowsRunAsUserName feature flag."', args=[d.arg(name='runAsUserName', type=d.T.string)]),
        withRunAsUserName(runAsUserName): { workflowSpec+: { securityContext+: { windowsOptions+: { runAsUserName: runAsUserName } } } },
      },
      '#withFsGroup':: d.fn(help="\"A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod:\\n\\n1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw----\\n\\nIf unset, the Kubelet will not modify the ownership and permissions of any volume.\"", args=[d.arg(name='fsGroup', type=d.T.integer)]),
      withFsGroup(fsGroup): { workflowSpec+: { securityContext+: { fsGroup: fsGroup } } },
      '#withRunAsGroup':: d.fn(help='"The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container."', args=[d.arg(name='runAsGroup', type=d.T.integer)]),
      withRunAsGroup(runAsGroup): { workflowSpec+: { securityContext+: { runAsGroup: runAsGroup } } },
      '#withRunAsNonRoot':: d.fn(help='"Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsNonRoot', type=d.T.boolean)]),
      withRunAsNonRoot(runAsNonRoot): { workflowSpec+: { securityContext+: { runAsNonRoot: runAsNonRoot } } },
      '#withRunAsUser':: d.fn(help='"The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container."', args=[d.arg(name='runAsUser', type=d.T.integer)]),
      withRunAsUser(runAsUser): { workflowSpec+: { securityContext+: { runAsUser: runAsUser } } },
      '#withSupplementalGroups':: d.fn(help="\"A list of groups applied to the first process run in each container, in addition to the container's primary GID.  If unspecified, no groups will be added to any container.\"", args=[d.arg(name='supplementalGroups', type=d.T.array)]),
      withSupplementalGroups(supplementalGroups): { workflowSpec+: { securityContext+: { supplementalGroups: if std.isArray(v=supplementalGroups) then supplementalGroups else [supplementalGroups] } } },
      '#withSupplementalGroupsMixin':: d.fn(help="\"A list of groups applied to the first process run in each container, in addition to the container's primary GID.  If unspecified, no groups will be added to any container.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='supplementalGroups', type=d.T.array)]),
      withSupplementalGroupsMixin(supplementalGroups): { workflowSpec+: { securityContext+: { supplementalGroups+: if std.isArray(v=supplementalGroups) then supplementalGroups else [supplementalGroups] } } },
      '#withSysctls':: d.fn(help='"Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch."', args=[d.arg(name='sysctls', type=d.T.array)]),
      withSysctls(sysctls): { workflowSpec+: { securityContext+: { sysctls: if std.isArray(v=sysctls) then sysctls else [sysctls] } } },
      '#withSysctlsMixin':: d.fn(help='"Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sysctls', type=d.T.array)]),
      withSysctlsMixin(sysctls): { workflowSpec+: { securityContext+: { sysctls+: if std.isArray(v=sysctls) then sysctls else [sysctls] } } },
    },
    '#synchronization':: d.obj(help='"Synchronization holds synchronization lock configuration"'),
    synchronization: {
      '#mutex':: d.obj(help='"Mutex holds Mutex configuration"'),
      mutex: {
        '#withName':: d.fn(help='"name of the mutex"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { workflowSpec+: { synchronization+: { mutex+: { name: name } } } },
      },
      '#semaphore':: d.obj(help='"SemaphoreRef is a reference of Semaphore"'),
      semaphore: {
        '#configMapKeyRef':: d.obj(help='"Selects a key from a ConfigMap."'),
        configMapKeyRef: {
          '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { workflowSpec+: { synchronization+: { semaphore+: { configMapKeyRef+: { key: key } } } } },
          '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { workflowSpec+: { synchronization+: { semaphore+: { configMapKeyRef+: { name: name } } } } },
          '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { workflowSpec+: { synchronization+: { semaphore+: { configMapKeyRef+: { optional: optional } } } } },
        },
      },
    },
    '#templateDefaults':: d.obj(help='"Template is a reusable and composable unit of execution in a workflow"'),
    templateDefaults: {
      '#affinity':: d.obj(help='"Affinity is a group of affinity scheduling rules."'),
      affinity: {
        '#nodeAffinity':: d.obj(help='"Node affinity is a group of node affinity scheduling rules."'),
        nodeAffinity: {
          '#requiredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms."'),
          requiredDuringSchedulingIgnoredDuringExecution: {
            '#withNodeSelectorTerms':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
            withNodeSelectorTerms(nodeSelectorTerms): { workflowSpec+: { templateDefaults+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } } },
            '#withNodeSelectorTermsMixin':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
            withNodeSelectorTermsMixin(nodeSelectorTerms): { workflowSpec+: { templateDefaults+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms+: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } } },
          },
          '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
        },
        '#podAffinity':: d.obj(help='"Pod affinity is a group of inter pod affinity scheduling rules."'),
        podAffinity: {
          '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } } },
        },
        '#podAntiAffinity':: d.obj(help='"Pod anti affinity is a group of inter pod anti affinity scheduling rules."'),
        podAntiAffinity: {
          '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } } },
          '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
          withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { workflowSpec+: { templateDefaults+: { affinity+: { podAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } } },
        },
      },
      '#archiveLocation':: d.obj(help='"ArtifactLocation describes a location for a single or multiple artifacts. It is used as single artifact in the context of inputs/outputs (e.g. outputs.artifacts.artname). It is also used to describe the location of multiple artifacts such as the archive location of a single workflow step, which the executor will use as a default location to store its files."'),
      archiveLocation: {
        '#artifactory':: d.obj(help='"ArtifactoryArtifact is the location of an artifactory artifact"'),
        artifactory: {
          '#passwordSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          passwordSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { passwordSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { passwordSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { passwordSecret+: { optional: optional } } } } } },
          },
          '#usernameSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          usernameSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { usernameSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { usernameSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { usernameSecret+: { optional: optional } } } } } },
          },
          '#withUrl':: d.fn(help='"URL of the artifact"', args=[d.arg(name='url', type=d.T.string)]),
          withUrl(url): { workflowSpec+: { templateDefaults+: { archiveLocation+: { artifactory+: { url: url } } } } },
        },
        '#gcs':: d.obj(help='"GCSArtifact is the location of a GCS artifact"'),
        gcs: {
          '#serviceAccountKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          serviceAccountKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { gcs+: { serviceAccountKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { gcs+: { serviceAccountKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { gcs+: { serviceAccountKeySecret+: { optional: optional } } } } } },
          },
          '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
          withBucket(bucket): { workflowSpec+: { templateDefaults+: { archiveLocation+: { gcs+: { bucket: bucket } } } } },
          '#withKey':: d.fn(help='"Key is the path in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { gcs+: { key: key } } } } },
        },
        '#git':: d.obj(help='"GitArtifact is the location of an git artifact"'),
        git: {
          '#passwordSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          passwordSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { passwordSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { passwordSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { passwordSecret+: { optional: optional } } } } } },
          },
          '#sshPrivateKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          sshPrivateKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { sshPrivateKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { sshPrivateKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { sshPrivateKeySecret+: { optional: optional } } } } } },
          },
          '#usernameSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          usernameSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { usernameSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { usernameSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { usernameSecret+: { optional: optional } } } } } },
          },
          '#withDepth':: d.fn(help='"Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip"', args=[d.arg(name='depth', type=d.T.integer)]),
          withDepth(depth): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { depth: depth } } } } },
          '#withDisableSubmodules':: d.fn(help='"DisableSubmodules disables submodules during git clone"', args=[d.arg(name='disableSubmodules', type=d.T.boolean)]),
          withDisableSubmodules(disableSubmodules): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { disableSubmodules: disableSubmodules } } } } },
          '#withFetch':: d.fn(help='"Fetch specifies a number of refs that should be fetched before checkout"', args=[d.arg(name='fetch', type=d.T.array)]),
          withFetch(fetch): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { fetch: if std.isArray(v=fetch) then fetch else [fetch] } } } } },
          '#withFetchMixin':: d.fn(help='"Fetch specifies a number of refs that should be fetched before checkout"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='fetch', type=d.T.array)]),
          withFetchMixin(fetch): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { fetch+: if std.isArray(v=fetch) then fetch else [fetch] } } } } },
          '#withInsecureIgnoreHostKey':: d.fn(help='"InsecureIgnoreHostKey disables SSH strict host key checking during git clone"', args=[d.arg(name='insecureIgnoreHostKey', type=d.T.boolean)]),
          withInsecureIgnoreHostKey(insecureIgnoreHostKey): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { insecureIgnoreHostKey: insecureIgnoreHostKey } } } } },
          '#withRepo':: d.fn(help='"Repo is the git repository"', args=[d.arg(name='repo', type=d.T.string)]),
          withRepo(repo): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { repo: repo } } } } },
          '#withRevision':: d.fn(help='"Revision is the git commit, tag, branch to checkout"', args=[d.arg(name='revision', type=d.T.string)]),
          withRevision(revision): { workflowSpec+: { templateDefaults+: { archiveLocation+: { git+: { revision: revision } } } } },
        },
        '#hdfs':: d.obj(help='"HDFSArtifact is the location of an HDFS artifact"'),
        hdfs: {
          '#krbCCacheSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          krbCCacheSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbCCacheSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbCCacheSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbCCacheSecret+: { optional: optional } } } } } },
          },
          '#krbConfigConfigMap':: d.obj(help='"Selects a key from a ConfigMap."'),
          krbConfigConfigMap: {
            '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbConfigConfigMap+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbConfigConfigMap+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbConfigConfigMap+: { optional: optional } } } } } },
          },
          '#krbKeytabSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          krbKeytabSecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbKeytabSecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbKeytabSecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbKeytabSecret+: { optional: optional } } } } } },
          },
          '#withAddresses':: d.fn(help='"Addresses is accessible addresses of HDFS name nodes"', args=[d.arg(name='addresses', type=d.T.array)]),
          withAddresses(addresses): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { addresses: if std.isArray(v=addresses) then addresses else [addresses] } } } } },
          '#withAddressesMixin':: d.fn(help='"Addresses is accessible addresses of HDFS name nodes"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='addresses', type=d.T.array)]),
          withAddressesMixin(addresses): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { addresses+: if std.isArray(v=addresses) then addresses else [addresses] } } } } },
          '#withForce':: d.fn(help='"Force copies a file forcibly even if it exists (default: false)"', args=[d.arg(name='force', type=d.T.boolean)]),
          withForce(force): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { force: force } } } } },
          '#withHdfsUser':: d.fn(help='"HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."', args=[d.arg(name='hdfsUser', type=d.T.string)]),
          withHdfsUser(hdfsUser): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { hdfsUser: hdfsUser } } } } },
          '#withKrbRealm':: d.fn(help='"KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used."', args=[d.arg(name='krbRealm', type=d.T.string)]),
          withKrbRealm(krbRealm): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbRealm: krbRealm } } } } },
          '#withKrbServicePrincipalName':: d.fn(help='"KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used."', args=[d.arg(name='krbServicePrincipalName', type=d.T.string)]),
          withKrbServicePrincipalName(krbServicePrincipalName): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbServicePrincipalName: krbServicePrincipalName } } } } },
          '#withKrbUsername':: d.fn(help='"KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used."', args=[d.arg(name='krbUsername', type=d.T.string)]),
          withKrbUsername(krbUsername): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { krbUsername: krbUsername } } } } },
          '#withPath':: d.fn(help='"Path is a file path in HDFS"', args=[d.arg(name='path', type=d.T.string)]),
          withPath(path): { workflowSpec+: { templateDefaults+: { archiveLocation+: { hdfs+: { path: path } } } } },
        },
        '#http':: d.obj(help='"HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container"'),
        http: {
          '#withHeaders':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests for artifacts"', args=[d.arg(name='headers', type=d.T.array)]),
          withHeaders(headers): { workflowSpec+: { templateDefaults+: { archiveLocation+: { http+: { headers: if std.isArray(v=headers) then headers else [headers] } } } } },
          '#withHeadersMixin':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests for artifacts"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headers', type=d.T.array)]),
          withHeadersMixin(headers): { workflowSpec+: { templateDefaults+: { archiveLocation+: { http+: { headers+: if std.isArray(v=headers) then headers else [headers] } } } } },
          '#withUrl':: d.fn(help='"URL of the artifact"', args=[d.arg(name='url', type=d.T.string)]),
          withUrl(url): { workflowSpec+: { templateDefaults+: { archiveLocation+: { http+: { url: url } } } } },
        },
        '#oss':: d.obj(help='"OSSArtifact is the location of an Alibaba Cloud OSS artifact"'),
        oss: {
          '#accessKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          accessKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { accessKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { accessKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { accessKeySecret+: { optional: optional } } } } } },
          },
          '#lifecycleRule':: d.obj(help="\"OSSLifecycleRule specifies how to manage bucket's lifecycle\""),
          lifecycleRule: {
            '#withMarkDeletionAfterDays':: d.fn(help='"MarkDeletionAfterDays is the number of days before we delete objects in the bucket"', args=[d.arg(name='markDeletionAfterDays', type=d.T.integer)]),
            withMarkDeletionAfterDays(markDeletionAfterDays): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { lifecycleRule+: { markDeletionAfterDays: markDeletionAfterDays } } } } } },
            '#withMarkInfrequentAccessAfterDays':: d.fn(help='"MarkInfrequentAccessAfterDays is the number of days before we convert the objects in the bucket to Infrequent Access (IA) storage type"', args=[d.arg(name='markInfrequentAccessAfterDays', type=d.T.integer)]),
            withMarkInfrequentAccessAfterDays(markInfrequentAccessAfterDays): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { lifecycleRule+: { markInfrequentAccessAfterDays: markInfrequentAccessAfterDays } } } } } },
          },
          '#secretKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          secretKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { secretKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { secretKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { secretKeySecret+: { optional: optional } } } } } },
          },
          '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
          withBucket(bucket): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { bucket: bucket } } } } },
          '#withCreateBucketIfNotPresent':: d.fn(help="\"CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it doesn't exist\"", args=[d.arg(name='createBucketIfNotPresent', type=d.T.boolean)]),
          withCreateBucketIfNotPresent(createBucketIfNotPresent): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { createBucketIfNotPresent: createBucketIfNotPresent } } } } },
          '#withEndpoint':: d.fn(help='"Endpoint is the hostname of the bucket endpoint"', args=[d.arg(name='endpoint', type=d.T.string)]),
          withEndpoint(endpoint): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { endpoint: endpoint } } } } },
          '#withKey':: d.fn(help='"Key is the path in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { key: key } } } } },
          '#withSecurityToken':: d.fn(help="\"SecurityToken is the user's temporary security token. For more details, check out: https://www.alibabacloud.com/help/doc-detail/100624.htm\"", args=[d.arg(name='securityToken', type=d.T.string)]),
          withSecurityToken(securityToken): { workflowSpec+: { templateDefaults+: { archiveLocation+: { oss+: { securityToken: securityToken } } } } },
        },
        '#raw':: d.obj(help='"RawArtifact allows raw string content to be placed as an artifact in a container"'),
        raw: {
          '#withData':: d.fn(help='"Data is the string contents of the artifact"', args=[d.arg(name='data', type=d.T.string)]),
          withData(data): { workflowSpec+: { templateDefaults+: { archiveLocation+: { raw+: { data: data } } } } },
        },
        '#s3':: d.obj(help='"S3Artifact is the location of an S3 artifact"'),
        s3: {
          '#accessKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          accessKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { accessKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { accessKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { accessKeySecret+: { optional: optional } } } } } },
          },
          '#createBucketIfNotPresent':: d.obj(help='"CreateS3BucketOptions options used to determine automatic automatic bucket-creation process"'),
          createBucketIfNotPresent: {
            '#withObjectLocking':: d.fn(help='"ObjectLocking Enable object locking"', args=[d.arg(name='objectLocking', type=d.T.boolean)]),
            withObjectLocking(objectLocking): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { createBucketIfNotPresent+: { objectLocking: objectLocking } } } } } },
          },
          '#encryptionOptions':: d.obj(help='"S3EncryptionOptions used to determine encryption options during s3 operations"'),
          encryptionOptions: {
            '#serverSideCustomerKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
            serverSideCustomerKeySecret: {
              '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { key: key } } } } } } },
              '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { name: name } } } } } } },
              '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
              withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { optional: optional } } } } } } },
            },
            '#withEnableEncryption':: d.fn(help='"EnableEncryption tells the driver to encrypt objects if set to true. If kmsKeyId and serverSideCustomerKeySecret are not set, SSE-S3 will be used"', args=[d.arg(name='enableEncryption', type=d.T.boolean)]),
            withEnableEncryption(enableEncryption): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { enableEncryption: enableEncryption } } } } } },
            '#withKmsEncryptionContext':: d.fn(help='"KmsEncryptionContext is a json blob that contains an encryption context. See https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context for more information"', args=[d.arg(name='kmsEncryptionContext', type=d.T.string)]),
            withKmsEncryptionContext(kmsEncryptionContext): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { kmsEncryptionContext: kmsEncryptionContext } } } } } },
            '#withKmsKeyId':: d.fn(help='"KMSKeyId tells the driver to encrypt the object using the specified KMS Key."', args=[d.arg(name='kmsKeyId', type=d.T.string)]),
            withKmsKeyId(kmsKeyId): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { encryptionOptions+: { kmsKeyId: kmsKeyId } } } } } },
          },
          '#secretKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
          secretKeySecret: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { secretKeySecret+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { secretKeySecret+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { secretKeySecret+: { optional: optional } } } } } },
          },
          '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
          withBucket(bucket): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { bucket: bucket } } } } },
          '#withEndpoint':: d.fn(help='"Endpoint is the hostname of the bucket endpoint"', args=[d.arg(name='endpoint', type=d.T.string)]),
          withEndpoint(endpoint): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { endpoint: endpoint } } } } },
          '#withInsecure':: d.fn(help='"Insecure will connect to the service with TLS"', args=[d.arg(name='insecure', type=d.T.boolean)]),
          withInsecure(insecure): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { insecure: insecure } } } } },
          '#withKey':: d.fn(help='"Key is the key in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { key: key } } } } },
          '#withRegion':: d.fn(help='"Region contains the optional bucket region"', args=[d.arg(name='region', type=d.T.string)]),
          withRegion(region): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { region: region } } } } },
          '#withRoleARN':: d.fn(help='"RoleARN is the Amazon Resource Name (ARN) of the role to assume."', args=[d.arg(name='roleARN', type=d.T.string)]),
          withRoleARN(roleARN): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { roleARN: roleARN } } } } },
          '#withUseSDKCreds':: d.fn(help='"UseSDKCreds tells the driver to figure out credentials based on sdk defaults."', args=[d.arg(name='useSDKCreds', type=d.T.boolean)]),
          withUseSDKCreds(useSDKCreds): { workflowSpec+: { templateDefaults+: { archiveLocation+: { s3+: { useSDKCreds: useSDKCreds } } } } },
        },
        '#withArchiveLogs':: d.fn(help='"ArchiveLogs indicates if the container logs should be archived"', args=[d.arg(name='archiveLogs', type=d.T.boolean)]),
        withArchiveLogs(archiveLogs): { workflowSpec+: { templateDefaults+: { archiveLocation+: { archiveLogs: archiveLogs } } } },
      },
      '#container':: d.obj(help='"A single application container that you want to run within a pod."'),
      container: {
        '#lifecycle':: d.obj(help='"Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted."'),
        lifecycle: {
          '#postStart':: d.obj(help='"Handler defines a specific action that should be taken"'),
          postStart: {
            '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
            exec: {
              '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } } },
              '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } } },
            },
            '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
            httpGet: {
              '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { host: host } } } } } } },
              '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
              withPath(path): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { path: path } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { port: port } } } } } } },
              '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
              withScheme(scheme): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { httpGet+: { scheme: scheme } } } } } } },
            },
            '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
            tcpSocket: {
              '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { tcpSocket+: { host: host } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { postStart+: { tcpSocket+: { port: port } } } } } } },
            },
          },
          '#preStop':: d.obj(help='"Handler defines a specific action that should be taken"'),
          preStop: {
            '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
            exec: {
              '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } } },
              '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } } },
            },
            '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
            httpGet: {
              '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { host: host } } } } } } },
              '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
              withPath(path): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { path: path } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { port: port } } } } } } },
              '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
              withScheme(scheme): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { httpGet+: { scheme: scheme } } } } } } },
            },
            '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
            tcpSocket: {
              '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { tcpSocket+: { host: host } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { container+: { lifecycle+: { preStop+: { tcpSocket+: { port: port } } } } } } },
            },
          },
        },
        '#livenessProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        livenessProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { container+: { livenessProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#readinessProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        readinessProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { container+: { readinessProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#resources':: d.obj(help='"ResourceRequirements describes the compute resource requirements."'),
        resources: {
          '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"', args=[d.arg(name='limits', type=d.T.object)]),
          withLimits(limits): { workflowSpec+: { templateDefaults+: { container+: { resources+: { limits: limits } } } } },
          '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
          withLimitsMixin(limits): { workflowSpec+: { templateDefaults+: { container+: { resources+: { limits+: limits } } } } },
          '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"', args=[d.arg(name='requests', type=d.T.object)]),
          withRequests(requests): { workflowSpec+: { templateDefaults+: { container+: { resources+: { requests: requests } } } } },
          '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
          withRequestsMixin(requests): { workflowSpec+: { templateDefaults+: { container+: { resources+: { requests+: requests } } } } },
        },
        '#securityContext':: d.obj(help='"SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence."'),
        securityContext: {
          '#capabilities':: d.obj(help='"Adds and removes POSIX capabilities from running containers."'),
          capabilities: {
            '#withAdd':: d.fn(help='"Added capabilities"', args=[d.arg(name='add', type=d.T.array)]),
            withAdd(add): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { capabilities+: { add: if std.isArray(v=add) then add else [add] } } } } } },
            '#withAddMixin':: d.fn(help='"Added capabilities"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='add', type=d.T.array)]),
            withAddMixin(add): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { capabilities+: { add+: if std.isArray(v=add) then add else [add] } } } } } },
            '#withDrop':: d.fn(help='"Removed capabilities"', args=[d.arg(name='drop', type=d.T.array)]),
            withDrop(drop): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { capabilities+: { drop: if std.isArray(v=drop) then drop else [drop] } } } } } },
            '#withDropMixin':: d.fn(help='"Removed capabilities"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='drop', type=d.T.array)]),
            withDropMixin(drop): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { capabilities+: { drop+: if std.isArray(v=drop) then drop else [drop] } } } } } },
          },
          '#seLinuxOptions':: d.obj(help='"SELinuxOptions are the labels to be applied to the container"'),
          seLinuxOptions: {
            '#withLevel':: d.fn(help='"Level is SELinux level label that applies to the container."', args=[d.arg(name='level', type=d.T.string)]),
            withLevel(level): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { seLinuxOptions+: { level: level } } } } } },
            '#withRole':: d.fn(help='"Role is a SELinux role label that applies to the container."', args=[d.arg(name='role', type=d.T.string)]),
            withRole(role): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { seLinuxOptions+: { role: role } } } } } },
            '#withType':: d.fn(help='"Type is a SELinux type label that applies to the container."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { seLinuxOptions+: { type: type } } } } } },
            '#withUser':: d.fn(help='"User is a SELinux user label that applies to the container."', args=[d.arg(name='user', type=d.T.string)]),
            withUser(user): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { seLinuxOptions+: { user: user } } } } } },
          },
          '#windowsOptions':: d.obj(help='"WindowsSecurityContextOptions contain Windows-specific options and credentials."'),
          windowsOptions: {
            '#withGmsaCredentialSpec':: d.fn(help='"GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpec', type=d.T.string)]),
            withGmsaCredentialSpec(gmsaCredentialSpec): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { windowsOptions+: { gmsaCredentialSpec: gmsaCredentialSpec } } } } } },
            '#withGmsaCredentialSpecName':: d.fn(help='"GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpecName', type=d.T.string)]),
            withGmsaCredentialSpecName(gmsaCredentialSpecName): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { windowsOptions+: { gmsaCredentialSpecName: gmsaCredentialSpecName } } } } } },
            '#withRunAsUserName':: d.fn(help='"The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. This field is beta-level and may be disabled with the WindowsRunAsUserName feature flag."', args=[d.arg(name='runAsUserName', type=d.T.string)]),
            withRunAsUserName(runAsUserName): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { windowsOptions+: { runAsUserName: runAsUserName } } } } } },
          },
          '#withAllowPrivilegeEscalation':: d.fn(help='"AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN"', args=[d.arg(name='allowPrivilegeEscalation', type=d.T.boolean)]),
          withAllowPrivilegeEscalation(allowPrivilegeEscalation): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { allowPrivilegeEscalation: allowPrivilegeEscalation } } } } },
          '#withPrivileged':: d.fn(help='"Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false."', args=[d.arg(name='privileged', type=d.T.boolean)]),
          withPrivileged(privileged): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { privileged: privileged } } } } },
          '#withProcMount':: d.fn(help='"procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled."', args=[d.arg(name='procMount', type=d.T.string)]),
          withProcMount(procMount): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { procMount: procMount } } } } },
          '#withReadOnlyRootFilesystem':: d.fn(help='"Whether this container has a read-only root filesystem. Default is false."', args=[d.arg(name='readOnlyRootFilesystem', type=d.T.boolean)]),
          withReadOnlyRootFilesystem(readOnlyRootFilesystem): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { readOnlyRootFilesystem: readOnlyRootFilesystem } } } } },
          '#withRunAsGroup':: d.fn(help='"The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsGroup', type=d.T.integer)]),
          withRunAsGroup(runAsGroup): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { runAsGroup: runAsGroup } } } } },
          '#withRunAsNonRoot':: d.fn(help='"Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsNonRoot', type=d.T.boolean)]),
          withRunAsNonRoot(runAsNonRoot): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { runAsNonRoot: runAsNonRoot } } } } },
          '#withRunAsUser':: d.fn(help='"The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsUser', type=d.T.integer)]),
          withRunAsUser(runAsUser): { workflowSpec+: { templateDefaults+: { container+: { securityContext+: { runAsUser: runAsUser } } } } },
        },
        '#startupProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        startupProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { container+: { startupProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#withArgs':: d.fn(help="\"Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"", args=[d.arg(name='args', type=d.T.array)]),
        withArgs(args): { workflowSpec+: { templateDefaults+: { container+: { args: if std.isArray(v=args) then args else [args] } } } },
        '#withArgsMixin':: d.fn(help="\"Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='args', type=d.T.array)]),
        withArgsMixin(args): { workflowSpec+: { templateDefaults+: { container+: { args+: if std.isArray(v=args) then args else [args] } } } },
        '#withCommand':: d.fn(help="\"Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"", args=[d.arg(name='command', type=d.T.array)]),
        withCommand(command): { workflowSpec+: { templateDefaults+: { container+: { command: if std.isArray(v=command) then command else [command] } } } },
        '#withCommandMixin':: d.fn(help="\"Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
        withCommandMixin(command): { workflowSpec+: { templateDefaults+: { container+: { command+: if std.isArray(v=command) then command else [command] } } } },
        '#withEnv':: d.fn(help='"List of environment variables to set in the container. Cannot be updated."', args=[d.arg(name='env', type=d.T.array)]),
        withEnv(env): { workflowSpec+: { templateDefaults+: { container+: { env: if std.isArray(v=env) then env else [env] } } } },
        '#withEnvFrom':: d.fn(help='"List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated."', args=[d.arg(name='envFrom', type=d.T.array)]),
        withEnvFrom(envFrom): { workflowSpec+: { templateDefaults+: { container+: { envFrom: if std.isArray(v=envFrom) then envFrom else [envFrom] } } } },
        '#withEnvFromMixin':: d.fn(help='"List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='envFrom', type=d.T.array)]),
        withEnvFromMixin(envFrom): { workflowSpec+: { templateDefaults+: { container+: { envFrom+: if std.isArray(v=envFrom) then envFrom else [envFrom] } } } },
        '#withEnvMixin':: d.fn(help='"List of environment variables to set in the container. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='env', type=d.T.array)]),
        withEnvMixin(env): { workflowSpec+: { templateDefaults+: { container+: { env+: if std.isArray(v=env) then env else [env] } } } },
        '#withImage':: d.fn(help='"Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets."', args=[d.arg(name='image', type=d.T.string)]),
        withImage(image): { workflowSpec+: { templateDefaults+: { container+: { image: image } } } },
        '#withImagePullPolicy':: d.fn(help='"Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images"', args=[d.arg(name='imagePullPolicy', type=d.T.string)]),
        withImagePullPolicy(imagePullPolicy): { workflowSpec+: { templateDefaults+: { container+: { imagePullPolicy: imagePullPolicy } } } },
        '#withName':: d.fn(help='"Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { workflowSpec+: { templateDefaults+: { container+: { name: name } } } },
        '#withPorts':: d.fn(help='"List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \\"0.0.0.0\\" address inside a container will be accessible from the network. Cannot be updated."', args=[d.arg(name='ports', type=d.T.array)]),
        withPorts(ports): { workflowSpec+: { templateDefaults+: { container+: { ports: if std.isArray(v=ports) then ports else [ports] } } } },
        '#withPortsMixin':: d.fn(help='"List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \\"0.0.0.0\\" address inside a container will be accessible from the network. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ports', type=d.T.array)]),
        withPortsMixin(ports): { workflowSpec+: { templateDefaults+: { container+: { ports+: if std.isArray(v=ports) then ports else [ports] } } } },
        '#withStdin':: d.fn(help='"Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false."', args=[d.arg(name='stdin', type=d.T.boolean)]),
        withStdin(stdin): { workflowSpec+: { templateDefaults+: { container+: { stdin: stdin } } } },
        '#withStdinOnce':: d.fn(help='"Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false"', args=[d.arg(name='stdinOnce', type=d.T.boolean)]),
        withStdinOnce(stdinOnce): { workflowSpec+: { templateDefaults+: { container+: { stdinOnce: stdinOnce } } } },
        '#withTerminationMessagePath':: d.fn(help="\"Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated.\"", args=[d.arg(name='terminationMessagePath', type=d.T.string)]),
        withTerminationMessagePath(terminationMessagePath): { workflowSpec+: { templateDefaults+: { container+: { terminationMessagePath: terminationMessagePath } } } },
        '#withTerminationMessagePolicy':: d.fn(help='"Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated."', args=[d.arg(name='terminationMessagePolicy', type=d.T.string)]),
        withTerminationMessagePolicy(terminationMessagePolicy): { workflowSpec+: { templateDefaults+: { container+: { terminationMessagePolicy: terminationMessagePolicy } } } },
        '#withTty':: d.fn(help="\"Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false.\"", args=[d.arg(name='tty', type=d.T.boolean)]),
        withTty(tty): { workflowSpec+: { templateDefaults+: { container+: { tty: tty } } } },
        '#withVolumeDevices':: d.fn(help='"volumeDevices is the list of block devices to be used by the container. This is a beta feature."', args=[d.arg(name='volumeDevices', type=d.T.array)]),
        withVolumeDevices(volumeDevices): { workflowSpec+: { templateDefaults+: { container+: { volumeDevices: if std.isArray(v=volumeDevices) then volumeDevices else [volumeDevices] } } } },
        '#withVolumeDevicesMixin':: d.fn(help='"volumeDevices is the list of block devices to be used by the container. This is a beta feature."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumeDevices', type=d.T.array)]),
        withVolumeDevicesMixin(volumeDevices): { workflowSpec+: { templateDefaults+: { container+: { volumeDevices+: if std.isArray(v=volumeDevices) then volumeDevices else [volumeDevices] } } } },
        '#withVolumeMounts':: d.fn(help="\"Pod volumes to mount into the container's filesystem. Cannot be updated.\"", args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMounts(volumeMounts): { workflowSpec+: { templateDefaults+: { container+: { volumeMounts: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
        '#withVolumeMountsMixin':: d.fn(help="\"Pod volumes to mount into the container's filesystem. Cannot be updated.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMountsMixin(volumeMounts): { workflowSpec+: { templateDefaults+: { container+: { volumeMounts+: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
        '#withWorkingDir':: d.fn(help="\"Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated.\"", args=[d.arg(name='workingDir', type=d.T.string)]),
        withWorkingDir(workingDir): { workflowSpec+: { templateDefaults+: { container+: { workingDir: workingDir } } } },
      },
      '#containerSet':: d.obj(help=''),
      containerSet: {
        '#withContainers':: d.fn(help='', args=[d.arg(name='containers', type=d.T.array)]),
        withContainers(containers): { workflowSpec+: { templateDefaults+: { containerSet+: { containers: if std.isArray(v=containers) then containers else [containers] } } } },
        '#withContainersMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='containers', type=d.T.array)]),
        withContainersMixin(containers): { workflowSpec+: { templateDefaults+: { containerSet+: { containers+: if std.isArray(v=containers) then containers else [containers] } } } },
        '#withVolumeMounts':: d.fn(help='', args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMounts(volumeMounts): { workflowSpec+: { templateDefaults+: { containerSet+: { volumeMounts: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
        '#withVolumeMountsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMountsMixin(volumeMounts): { workflowSpec+: { templateDefaults+: { containerSet+: { volumeMounts+: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
      },
      '#dag':: d.obj(help='"DAGTemplate is a template subtype for directed acyclic graph templates"'),
      dag: {
        '#withFailFast':: d.fn(help='"This flag is for DAG logic. The DAG logic has a built-in \\"fail fast\\" feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true,  if set to false, it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at https://github.com/argoproj/argo-workflows/issues/1442"', args=[d.arg(name='failFast', type=d.T.boolean)]),
        withFailFast(failFast): { workflowSpec+: { templateDefaults+: { dag+: { failFast: failFast } } } },
        '#withTarget':: d.fn(help='"Target are one or more names of targets to execute in a DAG"', args=[d.arg(name='target', type=d.T.string)]),
        withTarget(target): { workflowSpec+: { templateDefaults+: { dag+: { target: target } } } },
        '#withTasks':: d.fn(help='"Tasks are a list of DAG tasks"', args=[d.arg(name='tasks', type=d.T.array)]),
        withTasks(tasks): { workflowSpec+: { templateDefaults+: { dag+: { tasks: if std.isArray(v=tasks) then tasks else [tasks] } } } },
        '#withTasksMixin':: d.fn(help='"Tasks are a list of DAG tasks"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tasks', type=d.T.array)]),
        withTasksMixin(tasks): { workflowSpec+: { templateDefaults+: { dag+: { tasks+: if std.isArray(v=tasks) then tasks else [tasks] } } } },
      },
      '#data':: d.obj(help='"Data is a data template"'),
      data: {
        '#source':: d.obj(help='"DataSource sources external data into a data template"'),
        source: {
          '#artifactPaths':: d.obj(help='"ArtifactPaths expands a step from a collection of artifacts"'),
          artifactPaths: {
            '#archive':: d.obj(help='"ArchiveStrategy describes how to archive files/directory when saving artifacts"'),
            archive: {
              '#tar':: d.obj(help='"TarStrategy will tar and gzip the file or directory when saving"'),
              tar: {
                '#withCompressionLevel':: d.fn(help='"CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to gzip.DefaultCompression."', args=[d.arg(name='compressionLevel', type=d.T.integer)]),
                withCompressionLevel(compressionLevel): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archive+: { tar+: { compressionLevel: compressionLevel } } } } } } } },
              },
              '#withNone':: d.fn(help='"NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately."', args=[d.arg(name='none', type=d.T.object)]),
              withNone(none): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archive+: { none: none } } } } } } },
              '#withNoneMixin':: d.fn(help='"NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='none', type=d.T.object)]),
              withNoneMixin(none): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archive+: { none+: none } } } } } } },
              '#withZip':: d.fn(help='"ZipStrategy will unzip zipped input artifacts"', args=[d.arg(name='zip', type=d.T.object)]),
              withZip(zip): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archive+: { zip: zip } } } } } } },
              '#withZipMixin':: d.fn(help='"ZipStrategy will unzip zipped input artifacts"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='zip', type=d.T.object)]),
              withZipMixin(zip): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archive+: { zip+: zip } } } } } } },
            },
            '#artifactory':: d.obj(help='"ArtifactoryArtifact is the location of an artifactory artifact"'),
            artifactory: {
              '#passwordSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              passwordSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { passwordSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { passwordSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { passwordSecret+: { optional: optional } } } } } } } },
              },
              '#usernameSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              usernameSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { usernameSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { usernameSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { usernameSecret+: { optional: optional } } } } } } } },
              },
              '#withUrl':: d.fn(help='"URL of the artifact"', args=[d.arg(name='url', type=d.T.string)]),
              withUrl(url): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { artifactory+: { url: url } } } } } } },
            },
            '#gcs':: d.obj(help='"GCSArtifact is the location of a GCS artifact"'),
            gcs: {
              '#serviceAccountKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              serviceAccountKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { gcs+: { serviceAccountKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { gcs+: { serviceAccountKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { gcs+: { serviceAccountKeySecret+: { optional: optional } } } } } } } },
              },
              '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
              withBucket(bucket): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { gcs+: { bucket: bucket } } } } } } },
              '#withKey':: d.fn(help='"Key is the path in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { gcs+: { key: key } } } } } } },
            },
            '#git':: d.obj(help='"GitArtifact is the location of an git artifact"'),
            git: {
              '#passwordSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              passwordSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { passwordSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { passwordSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { passwordSecret+: { optional: optional } } } } } } } },
              },
              '#sshPrivateKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              sshPrivateKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { sshPrivateKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { sshPrivateKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { sshPrivateKeySecret+: { optional: optional } } } } } } } },
              },
              '#usernameSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              usernameSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { usernameSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { usernameSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { usernameSecret+: { optional: optional } } } } } } } },
              },
              '#withDepth':: d.fn(help='"Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip"', args=[d.arg(name='depth', type=d.T.integer)]),
              withDepth(depth): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { depth: depth } } } } } } },
              '#withDisableSubmodules':: d.fn(help='"DisableSubmodules disables submodules during git clone"', args=[d.arg(name='disableSubmodules', type=d.T.boolean)]),
              withDisableSubmodules(disableSubmodules): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { disableSubmodules: disableSubmodules } } } } } } },
              '#withFetch':: d.fn(help='"Fetch specifies a number of refs that should be fetched before checkout"', args=[d.arg(name='fetch', type=d.T.array)]),
              withFetch(fetch): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { fetch: if std.isArray(v=fetch) then fetch else [fetch] } } } } } } },
              '#withFetchMixin':: d.fn(help='"Fetch specifies a number of refs that should be fetched before checkout"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='fetch', type=d.T.array)]),
              withFetchMixin(fetch): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { fetch+: if std.isArray(v=fetch) then fetch else [fetch] } } } } } } },
              '#withInsecureIgnoreHostKey':: d.fn(help='"InsecureIgnoreHostKey disables SSH strict host key checking during git clone"', args=[d.arg(name='insecureIgnoreHostKey', type=d.T.boolean)]),
              withInsecureIgnoreHostKey(insecureIgnoreHostKey): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { insecureIgnoreHostKey: insecureIgnoreHostKey } } } } } } },
              '#withRepo':: d.fn(help='"Repo is the git repository"', args=[d.arg(name='repo', type=d.T.string)]),
              withRepo(repo): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { repo: repo } } } } } } },
              '#withRevision':: d.fn(help='"Revision is the git commit, tag, branch to checkout"', args=[d.arg(name='revision', type=d.T.string)]),
              withRevision(revision): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { git+: { revision: revision } } } } } } },
            },
            '#hdfs':: d.obj(help='"HDFSArtifact is the location of an HDFS artifact"'),
            hdfs: {
              '#krbCCacheSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              krbCCacheSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbCCacheSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbCCacheSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbCCacheSecret+: { optional: optional } } } } } } } },
              },
              '#krbConfigConfigMap':: d.obj(help='"Selects a key from a ConfigMap."'),
              krbConfigConfigMap: {
                '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbConfigConfigMap+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbConfigConfigMap+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbConfigConfigMap+: { optional: optional } } } } } } } },
              },
              '#krbKeytabSecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              krbKeytabSecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbKeytabSecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbKeytabSecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbKeytabSecret+: { optional: optional } } } } } } } },
              },
              '#withAddresses':: d.fn(help='"Addresses is accessible addresses of HDFS name nodes"', args=[d.arg(name='addresses', type=d.T.array)]),
              withAddresses(addresses): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { addresses: if std.isArray(v=addresses) then addresses else [addresses] } } } } } } },
              '#withAddressesMixin':: d.fn(help='"Addresses is accessible addresses of HDFS name nodes"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='addresses', type=d.T.array)]),
              withAddressesMixin(addresses): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { addresses+: if std.isArray(v=addresses) then addresses else [addresses] } } } } } } },
              '#withForce':: d.fn(help='"Force copies a file forcibly even if it exists (default: false)"', args=[d.arg(name='force', type=d.T.boolean)]),
              withForce(force): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { force: force } } } } } } },
              '#withHdfsUser':: d.fn(help='"HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."', args=[d.arg(name='hdfsUser', type=d.T.string)]),
              withHdfsUser(hdfsUser): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { hdfsUser: hdfsUser } } } } } } },
              '#withKrbRealm':: d.fn(help='"KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used."', args=[d.arg(name='krbRealm', type=d.T.string)]),
              withKrbRealm(krbRealm): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbRealm: krbRealm } } } } } } },
              '#withKrbServicePrincipalName':: d.fn(help='"KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used."', args=[d.arg(name='krbServicePrincipalName', type=d.T.string)]),
              withKrbServicePrincipalName(krbServicePrincipalName): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbServicePrincipalName: krbServicePrincipalName } } } } } } },
              '#withKrbUsername':: d.fn(help='"KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used."', args=[d.arg(name='krbUsername', type=d.T.string)]),
              withKrbUsername(krbUsername): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { krbUsername: krbUsername } } } } } } },
              '#withPath':: d.fn(help='"Path is a file path in HDFS"', args=[d.arg(name='path', type=d.T.string)]),
              withPath(path): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { hdfs+: { path: path } } } } } } },
            },
            '#http':: d.obj(help='"HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container"'),
            http: {
              '#withHeaders':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests for artifacts"', args=[d.arg(name='headers', type=d.T.array)]),
              withHeaders(headers): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { http+: { headers: if std.isArray(v=headers) then headers else [headers] } } } } } } },
              '#withHeadersMixin':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests for artifacts"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headers', type=d.T.array)]),
              withHeadersMixin(headers): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { http+: { headers+: if std.isArray(v=headers) then headers else [headers] } } } } } } },
              '#withUrl':: d.fn(help='"URL of the artifact"', args=[d.arg(name='url', type=d.T.string)]),
              withUrl(url): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { http+: { url: url } } } } } } },
            },
            '#oss':: d.obj(help='"OSSArtifact is the location of an Alibaba Cloud OSS artifact"'),
            oss: {
              '#accessKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              accessKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { accessKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { accessKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { accessKeySecret+: { optional: optional } } } } } } } },
              },
              '#lifecycleRule':: d.obj(help="\"OSSLifecycleRule specifies how to manage bucket's lifecycle\""),
              lifecycleRule: {
                '#withMarkDeletionAfterDays':: d.fn(help='"MarkDeletionAfterDays is the number of days before we delete objects in the bucket"', args=[d.arg(name='markDeletionAfterDays', type=d.T.integer)]),
                withMarkDeletionAfterDays(markDeletionAfterDays): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { lifecycleRule+: { markDeletionAfterDays: markDeletionAfterDays } } } } } } } },
                '#withMarkInfrequentAccessAfterDays':: d.fn(help='"MarkInfrequentAccessAfterDays is the number of days before we convert the objects in the bucket to Infrequent Access (IA) storage type"', args=[d.arg(name='markInfrequentAccessAfterDays', type=d.T.integer)]),
                withMarkInfrequentAccessAfterDays(markInfrequentAccessAfterDays): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { lifecycleRule+: { markInfrequentAccessAfterDays: markInfrequentAccessAfterDays } } } } } } } },
              },
              '#secretKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              secretKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { secretKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { secretKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { secretKeySecret+: { optional: optional } } } } } } } },
              },
              '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
              withBucket(bucket): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { bucket: bucket } } } } } } },
              '#withCreateBucketIfNotPresent':: d.fn(help="\"CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it doesn't exist\"", args=[d.arg(name='createBucketIfNotPresent', type=d.T.boolean)]),
              withCreateBucketIfNotPresent(createBucketIfNotPresent): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { createBucketIfNotPresent: createBucketIfNotPresent } } } } } } },
              '#withEndpoint':: d.fn(help='"Endpoint is the hostname of the bucket endpoint"', args=[d.arg(name='endpoint', type=d.T.string)]),
              withEndpoint(endpoint): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { endpoint: endpoint } } } } } } },
              '#withKey':: d.fn(help='"Key is the path in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { key: key } } } } } } },
              '#withSecurityToken':: d.fn(help="\"SecurityToken is the user's temporary security token. For more details, check out: https://www.alibabacloud.com/help/doc-detail/100624.htm\"", args=[d.arg(name='securityToken', type=d.T.string)]),
              withSecurityToken(securityToken): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { oss+: { securityToken: securityToken } } } } } } },
            },
            '#raw':: d.obj(help='"RawArtifact allows raw string content to be placed as an artifact in a container"'),
            raw: {
              '#withData':: d.fn(help='"Data is the string contents of the artifact"', args=[d.arg(name='data', type=d.T.string)]),
              withData(data): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { raw+: { data: data } } } } } } },
            },
            '#s3':: d.obj(help='"S3Artifact is the location of an S3 artifact"'),
            s3: {
              '#accessKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              accessKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { accessKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { accessKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { accessKeySecret+: { optional: optional } } } } } } } },
              },
              '#createBucketIfNotPresent':: d.obj(help='"CreateS3BucketOptions options used to determine automatic automatic bucket-creation process"'),
              createBucketIfNotPresent: {
                '#withObjectLocking':: d.fn(help='"ObjectLocking Enable object locking"', args=[d.arg(name='objectLocking', type=d.T.boolean)]),
                withObjectLocking(objectLocking): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { createBucketIfNotPresent+: { objectLocking: objectLocking } } } } } } } },
              },
              '#encryptionOptions':: d.obj(help='"S3EncryptionOptions used to determine encryption options during s3 operations"'),
              encryptionOptions: {
                '#serverSideCustomerKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
                serverSideCustomerKeySecret: {
                  '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                  withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { key: key } } } } } } } } },
                  '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                  withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { name: name } } } } } } } } },
                  '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                  withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { serverSideCustomerKeySecret+: { optional: optional } } } } } } } } },
                },
                '#withEnableEncryption':: d.fn(help='"EnableEncryption tells the driver to encrypt objects if set to true. If kmsKeyId and serverSideCustomerKeySecret are not set, SSE-S3 will be used"', args=[d.arg(name='enableEncryption', type=d.T.boolean)]),
                withEnableEncryption(enableEncryption): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { enableEncryption: enableEncryption } } } } } } } },
                '#withKmsEncryptionContext':: d.fn(help='"KmsEncryptionContext is a json blob that contains an encryption context. See https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context for more information"', args=[d.arg(name='kmsEncryptionContext', type=d.T.string)]),
                withKmsEncryptionContext(kmsEncryptionContext): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { kmsEncryptionContext: kmsEncryptionContext } } } } } } } },
                '#withKmsKeyId':: d.fn(help='"KMSKeyId tells the driver to encrypt the object using the specified KMS Key."', args=[d.arg(name='kmsKeyId', type=d.T.string)]),
                withKmsKeyId(kmsKeyId): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { encryptionOptions+: { kmsKeyId: kmsKeyId } } } } } } } },
              },
              '#secretKeySecret':: d.obj(help='"SecretKeySelector selects a key of a Secret."'),
              secretKeySecret: {
                '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { secretKeySecret+: { key: key } } } } } } } },
                '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { secretKeySecret+: { name: name } } } } } } } },
                '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
                withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { secretKeySecret+: { optional: optional } } } } } } } },
              },
              '#withBucket':: d.fn(help='"Bucket is the name of the bucket"', args=[d.arg(name='bucket', type=d.T.string)]),
              withBucket(bucket): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { bucket: bucket } } } } } } },
              '#withEndpoint':: d.fn(help='"Endpoint is the hostname of the bucket endpoint"', args=[d.arg(name='endpoint', type=d.T.string)]),
              withEndpoint(endpoint): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { endpoint: endpoint } } } } } } },
              '#withInsecure':: d.fn(help='"Insecure will connect to the service with TLS"', args=[d.arg(name='insecure', type=d.T.boolean)]),
              withInsecure(insecure): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { insecure: insecure } } } } } } },
              '#withKey':: d.fn(help='"Key is the key in the bucket where the artifact resides"', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { key: key } } } } } } },
              '#withRegion':: d.fn(help='"Region contains the optional bucket region"', args=[d.arg(name='region', type=d.T.string)]),
              withRegion(region): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { region: region } } } } } } },
              '#withRoleARN':: d.fn(help='"RoleARN is the Amazon Resource Name (ARN) of the role to assume."', args=[d.arg(name='roleARN', type=d.T.string)]),
              withRoleARN(roleARN): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { roleARN: roleARN } } } } } } },
              '#withUseSDKCreds':: d.fn(help='"UseSDKCreds tells the driver to figure out credentials based on sdk defaults."', args=[d.arg(name='useSDKCreds', type=d.T.boolean)]),
              withUseSDKCreds(useSDKCreds): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { s3+: { useSDKCreds: useSDKCreds } } } } } } },
            },
            '#withArchiveLogs':: d.fn(help='"ArchiveLogs indicates if the container logs should be archived"', args=[d.arg(name='archiveLogs', type=d.T.boolean)]),
            withArchiveLogs(archiveLogs): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { archiveLogs: archiveLogs } } } } } },
            '#withFrom':: d.fn(help='"From allows an artifact to reference an artifact from a previous step"', args=[d.arg(name='from', type=d.T.string)]),
            withFrom(from): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { from: from } } } } } },
            '#withFromExpression':: d.fn(help='"FromExpression, if defined, is evaluated to specify the value for the artifact"', args=[d.arg(name='fromExpression', type=d.T.string)]),
            withFromExpression(fromExpression): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { fromExpression: fromExpression } } } } } },
            '#withGlobalName':: d.fn(help="\"GlobalName exports an output artifact to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts\"", args=[d.arg(name='globalName', type=d.T.string)]),
            withGlobalName(globalName): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { globalName: globalName } } } } } },
            '#withMode':: d.fn(help='"mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts."', args=[d.arg(name='mode', type=d.T.integer)]),
            withMode(mode): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { mode: mode } } } } } },
            '#withName':: d.fn(help="\"name of the artifact. must be unique within a template's inputs/outputs.\"", args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { name: name } } } } } },
            '#withOptional':: d.fn(help="\"Make Artifacts optional, if Artifacts doesn't generate or exist\"", args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { optional: optional } } } } } },
            '#withPath':: d.fn(help='"Path is the container path to the artifact"', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { path: path } } } } } },
            '#withRecurseMode':: d.fn(help='"If mode is set, apply the permission recursively into the artifact if it is a folder"', args=[d.arg(name='recurseMode', type=d.T.boolean)]),
            withRecurseMode(recurseMode): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { recurseMode: recurseMode } } } } } },
            '#withSubPath':: d.fn(help='"SubPath allows an artifact to be sourced from a subpath within the specified source"', args=[d.arg(name='subPath', type=d.T.string)]),
            withSubPath(subPath): { workflowSpec+: { templateDefaults+: { data+: { source+: { artifactPaths+: { subPath: subPath } } } } } },
          },
        },
        '#withTransformation':: d.fn(help='"Transformation applies a set of transformations"', args=[d.arg(name='transformation', type=d.T.array)]),
        withTransformation(transformation): { workflowSpec+: { templateDefaults+: { data+: { transformation: if std.isArray(v=transformation) then transformation else [transformation] } } } },
        '#withTransformationMixin':: d.fn(help='"Transformation applies a set of transformations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='transformation', type=d.T.array)]),
        withTransformationMixin(transformation): { workflowSpec+: { templateDefaults+: { data+: { transformation+: if std.isArray(v=transformation) then transformation else [transformation] } } } },
      },
      '#executor':: d.obj(help='"ExecutorConfig holds configurations of an executor container."'),
      executor: {
        '#withServiceAccountName':: d.fn(help='"ServiceAccountName specifies the service account name of the executor container."', args=[d.arg(name='serviceAccountName', type=d.T.string)]),
        withServiceAccountName(serviceAccountName): { workflowSpec+: { templateDefaults+: { executor+: { serviceAccountName: serviceAccountName } } } },
      },
      '#http':: d.obj(help=''),
      http: {
        '#withBody':: d.fn(help='"Body is content of the HTTP Request"', args=[d.arg(name='body', type=d.T.string)]),
        withBody(body): { workflowSpec+: { templateDefaults+: { http+: { body: body } } } },
        '#withHeaders':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests"', args=[d.arg(name='headers', type=d.T.array)]),
        withHeaders(headers): { workflowSpec+: { templateDefaults+: { http+: { headers: if std.isArray(v=headers) then headers else [headers] } } } },
        '#withHeadersMixin':: d.fn(help='"Headers are an optional list of headers to send with HTTP requests"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headers', type=d.T.array)]),
        withHeadersMixin(headers): { workflowSpec+: { templateDefaults+: { http+: { headers+: if std.isArray(v=headers) then headers else [headers] } } } },
        '#withMethod':: d.fn(help='"Method is HTTP methods for HTTP Request"', args=[d.arg(name='method', type=d.T.string)]),
        withMethod(method): { workflowSpec+: { templateDefaults+: { http+: { method: method } } } },
        '#withTimeoutSeconds':: d.fn(help='"TimeoutSeconds is request timeout for HTTP Request. Default is 30 seconds"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
        withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { http+: { timeoutSeconds: timeoutSeconds } } } },
        '#withUrl':: d.fn(help='"URL of the HTTP Request"', args=[d.arg(name='url', type=d.T.string)]),
        withUrl(url): { workflowSpec+: { templateDefaults+: { http+: { url: url } } } },
      },
      '#inputs':: d.obj(help='"Inputs are the mechanism for passing parameters, artifacts, volumes from one template to another"'),
      inputs: {
        '#withArtifacts':: d.fn(help='"Artifact are a list of artifacts passed as inputs"', args=[d.arg(name='artifacts', type=d.T.array)]),
        withArtifacts(artifacts): { workflowSpec+: { templateDefaults+: { inputs+: { artifacts: if std.isArray(v=artifacts) then artifacts else [artifacts] } } } },
        '#withArtifactsMixin':: d.fn(help='"Artifact are a list of artifacts passed as inputs"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='artifacts', type=d.T.array)]),
        withArtifactsMixin(artifacts): { workflowSpec+: { templateDefaults+: { inputs+: { artifacts+: if std.isArray(v=artifacts) then artifacts else [artifacts] } } } },
        '#withParameters':: d.fn(help='"Parameters are a list of parameters passed as inputs"', args=[d.arg(name='parameters', type=d.T.array)]),
        withParameters(parameters): { workflowSpec+: { templateDefaults+: { inputs+: { parameters: if std.isArray(v=parameters) then parameters else [parameters] } } } },
        '#withParametersMixin':: d.fn(help='"Parameters are a list of parameters passed as inputs"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
        withParametersMixin(parameters): { workflowSpec+: { templateDefaults+: { inputs+: { parameters+: if std.isArray(v=parameters) then parameters else [parameters] } } } },
      },
      '#memoize':: d.obj(help='"Memoization enables caching for the Outputs of the template"'),
      memoize: {
        '#cache':: d.obj(help='"Cache is the configuration for the type of cache to be used"'),
        cache: {
          '#configMap':: d.obj(help='"Selects a key from a ConfigMap."'),
          configMap: {
            '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { memoize+: { cache+: { configMap+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { memoize+: { cache+: { configMap+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { memoize+: { cache+: { configMap+: { optional: optional } } } } } },
          },
        },
        '#withKey':: d.fn(help='"Key is the key to use as the caching key"', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { workflowSpec+: { templateDefaults+: { memoize+: { key: key } } } },
        '#withMaxAge':: d.fn(help='"MaxAge is the maximum age (e.g. \\"180s\\", \\"24h\\") of an entry that is still considered valid. If an entry is older than the MaxAge, it will be ignored."', args=[d.arg(name='maxAge', type=d.T.string)]),
        withMaxAge(maxAge): { workflowSpec+: { templateDefaults+: { memoize+: { maxAge: maxAge } } } },
      },
      '#metadata':: d.obj(help='"Pod metdata"'),
      metadata: {
        '#withAnnotations':: d.fn(help='', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotations(annotations): { workflowSpec+: { templateDefaults+: { metadata+: { annotations: annotations } } } },
        '#withAnnotationsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotationsMixin(annotations): { workflowSpec+: { templateDefaults+: { metadata+: { annotations+: annotations } } } },
        '#withLabels':: d.fn(help='', args=[d.arg(name='labels', type=d.T.object)]),
        withLabels(labels): { workflowSpec+: { templateDefaults+: { metadata+: { labels: labels } } } },
        '#withLabelsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
        withLabelsMixin(labels): { workflowSpec+: { templateDefaults+: { metadata+: { labels+: labels } } } },
      },
      '#metrics':: d.obj(help='"Metrics are a list of metrics emitted from a Workflow/Template"'),
      metrics: {
        '#withPrometheus':: d.fn(help='"Prometheus is a list of prometheus metrics to be emitted"', args=[d.arg(name='prometheus', type=d.T.array)]),
        withPrometheus(prometheus): { workflowSpec+: { templateDefaults+: { metrics+: { prometheus: if std.isArray(v=prometheus) then prometheus else [prometheus] } } } },
        '#withPrometheusMixin':: d.fn(help='"Prometheus is a list of prometheus metrics to be emitted"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='prometheus', type=d.T.array)]),
        withPrometheusMixin(prometheus): { workflowSpec+: { templateDefaults+: { metrics+: { prometheus+: if std.isArray(v=prometheus) then prometheus else [prometheus] } } } },
      },
      '#outputs':: d.obj(help='"Outputs hold parameters, artifacts, and results from a step"'),
      outputs: {
        '#withArtifacts':: d.fn(help='"Artifacts holds the list of output artifacts produced by a step"', args=[d.arg(name='artifacts', type=d.T.array)]),
        withArtifacts(artifacts): { workflowSpec+: { templateDefaults+: { outputs+: { artifacts: if std.isArray(v=artifacts) then artifacts else [artifacts] } } } },
        '#withArtifactsMixin':: d.fn(help='"Artifacts holds the list of output artifacts produced by a step"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='artifacts', type=d.T.array)]),
        withArtifactsMixin(artifacts): { workflowSpec+: { templateDefaults+: { outputs+: { artifacts+: if std.isArray(v=artifacts) then artifacts else [artifacts] } } } },
        '#withExitCode':: d.fn(help='"ExitCode holds the exit code of a script template"', args=[d.arg(name='exitCode', type=d.T.string)]),
        withExitCode(exitCode): { workflowSpec+: { templateDefaults+: { outputs+: { exitCode: exitCode } } } },
        '#withParameters':: d.fn(help='"Parameters holds the list of output parameters produced by a step"', args=[d.arg(name='parameters', type=d.T.array)]),
        withParameters(parameters): { workflowSpec+: { templateDefaults+: { outputs+: { parameters: if std.isArray(v=parameters) then parameters else [parameters] } } } },
        '#withParametersMixin':: d.fn(help='"Parameters holds the list of output parameters produced by a step"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
        withParametersMixin(parameters): { workflowSpec+: { templateDefaults+: { outputs+: { parameters+: if std.isArray(v=parameters) then parameters else [parameters] } } } },
        '#withResult':: d.fn(help='"Result holds the result (stdout) of a script template"', args=[d.arg(name='result', type=d.T.string)]),
        withResult(result): { workflowSpec+: { templateDefaults+: { outputs+: { result: result } } } },
      },
      '#resource':: d.obj(help='"ResourceTemplate is a template subtype to manipulate kubernetes resources"'),
      resource: {
        '#withAction':: d.fn(help='"Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace, patch"', args=[d.arg(name='action', type=d.T.string)]),
        withAction(action): { workflowSpec+: { templateDefaults+: { resource+: { action: action } } } },
        '#withFailureCondition':: d.fn(help='"FailureCondition is a label selector expression which describes the conditions of the k8s resource in which the step was considered failed"', args=[d.arg(name='failureCondition', type=d.T.string)]),
        withFailureCondition(failureCondition): { workflowSpec+: { templateDefaults+: { resource+: { failureCondition: failureCondition } } } },
        '#withFlags':: d.fn(help='"Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [\\n\\t\\"--validate=false\\"  # disable resource validation\\n]"', args=[d.arg(name='flags', type=d.T.array)]),
        withFlags(flags): { workflowSpec+: { templateDefaults+: { resource+: { flags: if std.isArray(v=flags) then flags else [flags] } } } },
        '#withFlagsMixin':: d.fn(help='"Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [\\n\\t\\"--validate=false\\"  # disable resource validation\\n]"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='flags', type=d.T.array)]),
        withFlagsMixin(flags): { workflowSpec+: { templateDefaults+: { resource+: { flags+: if std.isArray(v=flags) then flags else [flags] } } } },
        '#withManifest':: d.fn(help='"Manifest contains the kubernetes manifest"', args=[d.arg(name='manifest', type=d.T.string)]),
        withManifest(manifest): { workflowSpec+: { templateDefaults+: { resource+: { manifest: manifest } } } },
        '#withMergeStrategy':: d.fn(help='"MergeStrategy is the strategy used to merge a patch. It defaults to \\"strategic\\" Must be one of: strategic, merge, json"', args=[d.arg(name='mergeStrategy', type=d.T.string)]),
        withMergeStrategy(mergeStrategy): { workflowSpec+: { templateDefaults+: { resource+: { mergeStrategy: mergeStrategy } } } },
        '#withSetOwnerReference':: d.fn(help='"SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource."', args=[d.arg(name='setOwnerReference', type=d.T.boolean)]),
        withSetOwnerReference(setOwnerReference): { workflowSpec+: { templateDefaults+: { resource+: { setOwnerReference: setOwnerReference } } } },
        '#withSuccessCondition':: d.fn(help='"SuccessCondition is a label selector expression which describes the conditions of the k8s resource in which it is acceptable to proceed to the following step"', args=[d.arg(name='successCondition', type=d.T.string)]),
        withSuccessCondition(successCondition): { workflowSpec+: { templateDefaults+: { resource+: { successCondition: successCondition } } } },
      },
      '#retryStrategy':: d.obj(help='"RetryStrategy provides controls on how to retry a workflow step"'),
      retryStrategy: {
        '#affinity':: d.obj(help='"RetryAffinity prevents running steps on the same host."'),
        affinity: {
          '#withNodeAntiAffinity':: d.fn(help='"RetryNodeAntiAffinity is a placeholder for future expansion, only empty nodeAntiAffinity is allowed. In order to prevent running steps on the same host, it uses \\"kubernetes.io/hostname\\"."', args=[d.arg(name='nodeAntiAffinity', type=d.T.object)]),
          withNodeAntiAffinity(nodeAntiAffinity): { workflowSpec+: { templateDefaults+: { retryStrategy+: { affinity+: { nodeAntiAffinity: nodeAntiAffinity } } } } },
          '#withNodeAntiAffinityMixin':: d.fn(help='"RetryNodeAntiAffinity is a placeholder for future expansion, only empty nodeAntiAffinity is allowed. In order to prevent running steps on the same host, it uses \\"kubernetes.io/hostname\\"."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeAntiAffinity', type=d.T.object)]),
          withNodeAntiAffinityMixin(nodeAntiAffinity): { workflowSpec+: { templateDefaults+: { retryStrategy+: { affinity+: { nodeAntiAffinity+: nodeAntiAffinity } } } } },
        },
        '#backoff':: d.obj(help='"Backoff is a backoff strategy to use within retryStrategy"'),
        backoff: {
          '#withDuration':: d.fn(help='"Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \\"2m\\", \\"1h\\")"', args=[d.arg(name='duration', type=d.T.string)]),
          withDuration(duration): { workflowSpec+: { templateDefaults+: { retryStrategy+: { backoff+: { duration: duration } } } } },
          '#withFactor':: d.fn(help='', args=[d.arg(name='factor', type=d.T.string)]),
          withFactor(factor): { workflowSpec+: { templateDefaults+: { retryStrategy+: { backoff+: { factor: factor } } } } },
          '#withMaxDuration':: d.fn(help='"MaxDuration is the maximum amount of time allowed for the backoff strategy"', args=[d.arg(name='maxDuration', type=d.T.string)]),
          withMaxDuration(maxDuration): { workflowSpec+: { templateDefaults+: { retryStrategy+: { backoff+: { maxDuration: maxDuration } } } } },
        },
        '#withExpression':: d.fn(help='"Expression is a condition expression for when a node will be retried. If it evaluates to false, the node will not be retried and the retry strategy will be ignored/"', args=[d.arg(name='expression', type=d.T.string)]),
        withExpression(expression): { workflowSpec+: { templateDefaults+: { retryStrategy+: { expression: expression } } } },
        '#withLimit':: d.fn(help='', args=[d.arg(name='limit', type=d.T.string)]),
        withLimit(limit): { workflowSpec+: { templateDefaults+: { retryStrategy+: { limit: limit } } } },
        '#withRetryPolicy':: d.fn(help='"RetryPolicy is a policy of NodePhase statuses that will be retried"', args=[d.arg(name='retryPolicy', type=d.T.string)]),
        withRetryPolicy(retryPolicy): { workflowSpec+: { templateDefaults+: { retryStrategy+: { retryPolicy: retryPolicy } } } },
      },
      '#script':: d.obj(help='"ScriptTemplate is a template subtype to enable scripting through code steps"'),
      script: {
        '#lifecycle':: d.obj(help='"Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted."'),
        lifecycle: {
          '#postStart':: d.obj(help='"Handler defines a specific action that should be taken"'),
          postStart: {
            '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
            exec: {
              '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } } },
              '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } } },
            },
            '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
            httpGet: {
              '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { host: host } } } } } } },
              '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
              withPath(path): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { path: path } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { port: port } } } } } } },
              '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
              withScheme(scheme): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { httpGet+: { scheme: scheme } } } } } } },
            },
            '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
            tcpSocket: {
              '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { tcpSocket+: { host: host } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { postStart+: { tcpSocket+: { port: port } } } } } } },
            },
          },
          '#preStop':: d.obj(help='"Handler defines a specific action that should be taken"'),
          preStop: {
            '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
            exec: {
              '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } } },
              '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } } },
            },
            '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
            httpGet: {
              '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { host: host } } } } } } },
              '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
              withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } } },
              '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
              withPath(path): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { path: path } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { port: port } } } } } } },
              '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
              withScheme(scheme): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { httpGet+: { scheme: scheme } } } } } } },
            },
            '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
            tcpSocket: {
              '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
              withHost(host): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { tcpSocket+: { host: host } } } } } } },
              '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
              withPort(port): { workflowSpec+: { templateDefaults+: { script+: { lifecycle+: { preStop+: { tcpSocket+: { port: port } } } } } } },
            },
          },
        },
        '#livenessProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        livenessProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { script+: { livenessProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#readinessProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        readinessProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { script+: { readinessProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#resources':: d.obj(help='"ResourceRequirements describes the compute resource requirements."'),
        resources: {
          '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"', args=[d.arg(name='limits', type=d.T.object)]),
          withLimits(limits): { workflowSpec+: { templateDefaults+: { script+: { resources+: { limits: limits } } } } },
          '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
          withLimitsMixin(limits): { workflowSpec+: { templateDefaults+: { script+: { resources+: { limits+: limits } } } } },
          '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"', args=[d.arg(name='requests', type=d.T.object)]),
          withRequests(requests): { workflowSpec+: { templateDefaults+: { script+: { resources+: { requests: requests } } } } },
          '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
          withRequestsMixin(requests): { workflowSpec+: { templateDefaults+: { script+: { resources+: { requests+: requests } } } } },
        },
        '#securityContext':: d.obj(help='"SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence."'),
        securityContext: {
          '#capabilities':: d.obj(help='"Adds and removes POSIX capabilities from running containers."'),
          capabilities: {
            '#withAdd':: d.fn(help='"Added capabilities"', args=[d.arg(name='add', type=d.T.array)]),
            withAdd(add): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { capabilities+: { add: if std.isArray(v=add) then add else [add] } } } } } },
            '#withAddMixin':: d.fn(help='"Added capabilities"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='add', type=d.T.array)]),
            withAddMixin(add): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { capabilities+: { add+: if std.isArray(v=add) then add else [add] } } } } } },
            '#withDrop':: d.fn(help='"Removed capabilities"', args=[d.arg(name='drop', type=d.T.array)]),
            withDrop(drop): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { capabilities+: { drop: if std.isArray(v=drop) then drop else [drop] } } } } } },
            '#withDropMixin':: d.fn(help='"Removed capabilities"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='drop', type=d.T.array)]),
            withDropMixin(drop): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { capabilities+: { drop+: if std.isArray(v=drop) then drop else [drop] } } } } } },
          },
          '#seLinuxOptions':: d.obj(help='"SELinuxOptions are the labels to be applied to the container"'),
          seLinuxOptions: {
            '#withLevel':: d.fn(help='"Level is SELinux level label that applies to the container."', args=[d.arg(name='level', type=d.T.string)]),
            withLevel(level): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { seLinuxOptions+: { level: level } } } } } },
            '#withRole':: d.fn(help='"Role is a SELinux role label that applies to the container."', args=[d.arg(name='role', type=d.T.string)]),
            withRole(role): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { seLinuxOptions+: { role: role } } } } } },
            '#withType':: d.fn(help='"Type is a SELinux type label that applies to the container."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { seLinuxOptions+: { type: type } } } } } },
            '#withUser':: d.fn(help='"User is a SELinux user label that applies to the container."', args=[d.arg(name='user', type=d.T.string)]),
            withUser(user): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { seLinuxOptions+: { user: user } } } } } },
          },
          '#windowsOptions':: d.obj(help='"WindowsSecurityContextOptions contain Windows-specific options and credentials."'),
          windowsOptions: {
            '#withGmsaCredentialSpec':: d.fn(help='"GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpec', type=d.T.string)]),
            withGmsaCredentialSpec(gmsaCredentialSpec): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { windowsOptions+: { gmsaCredentialSpec: gmsaCredentialSpec } } } } } },
            '#withGmsaCredentialSpecName':: d.fn(help='"GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpecName', type=d.T.string)]),
            withGmsaCredentialSpecName(gmsaCredentialSpecName): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { windowsOptions+: { gmsaCredentialSpecName: gmsaCredentialSpecName } } } } } },
            '#withRunAsUserName':: d.fn(help='"The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. This field is beta-level and may be disabled with the WindowsRunAsUserName feature flag."', args=[d.arg(name='runAsUserName', type=d.T.string)]),
            withRunAsUserName(runAsUserName): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { windowsOptions+: { runAsUserName: runAsUserName } } } } } },
          },
          '#withAllowPrivilegeEscalation':: d.fn(help='"AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN"', args=[d.arg(name='allowPrivilegeEscalation', type=d.T.boolean)]),
          withAllowPrivilegeEscalation(allowPrivilegeEscalation): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { allowPrivilegeEscalation: allowPrivilegeEscalation } } } } },
          '#withPrivileged':: d.fn(help='"Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false."', args=[d.arg(name='privileged', type=d.T.boolean)]),
          withPrivileged(privileged): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { privileged: privileged } } } } },
          '#withProcMount':: d.fn(help='"procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled."', args=[d.arg(name='procMount', type=d.T.string)]),
          withProcMount(procMount): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { procMount: procMount } } } } },
          '#withReadOnlyRootFilesystem':: d.fn(help='"Whether this container has a read-only root filesystem. Default is false."', args=[d.arg(name='readOnlyRootFilesystem', type=d.T.boolean)]),
          withReadOnlyRootFilesystem(readOnlyRootFilesystem): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { readOnlyRootFilesystem: readOnlyRootFilesystem } } } } },
          '#withRunAsGroup':: d.fn(help='"The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsGroup', type=d.T.integer)]),
          withRunAsGroup(runAsGroup): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { runAsGroup: runAsGroup } } } } },
          '#withRunAsNonRoot':: d.fn(help='"Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsNonRoot', type=d.T.boolean)]),
          withRunAsNonRoot(runAsNonRoot): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { runAsNonRoot: runAsNonRoot } } } } },
          '#withRunAsUser':: d.fn(help='"The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsUser', type=d.T.integer)]),
          withRunAsUser(runAsUser): { workflowSpec+: { templateDefaults+: { script+: { securityContext+: { runAsUser: runAsUser } } } } },
        },
        '#startupProbe':: d.obj(help='"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic."'),
        startupProbe: {
          '#exec':: d.obj(help='"ExecAction describes a \\"run in container\\" action."'),
          exec: {
            '#withCommand':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"", args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { exec+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help="\"Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { exec+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
          },
          '#httpGet':: d.obj(help='"HTTPGetAction describes an action based on HTTP Get requests."'),
          httpGet: {
            '#withHost':: d.fn(help='"Host name to connect to, defaults to the pod IP. You probably want to set \\"Host\\" in httpHeaders instead."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { host: host } } } } } },
            '#withHttpHeaders':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeaders(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { httpHeaders: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withHttpHeadersMixin':: d.fn(help='"Custom headers to set in the request. HTTP allows repeated headers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpHeaders', type=d.T.array)]),
            withHttpHeadersMixin(httpHeaders): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { httpHeaders+: if std.isArray(v=httpHeaders) then httpHeaders else [httpHeaders] } } } } } },
            '#withPath':: d.fn(help='"Path to access on the HTTP server."', args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { path: path } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { port: port } } } } } },
            '#withScheme':: d.fn(help='"Scheme to use for connecting to the host. Defaults to HTTP."', args=[d.arg(name='scheme', type=d.T.string)]),
            withScheme(scheme): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { httpGet+: { scheme: scheme } } } } } },
          },
          '#tcpSocket':: d.obj(help='"TCPSocketAction describes an action based on opening a socket"'),
          tcpSocket: {
            '#withHost':: d.fn(help='"Optional: Host name to connect to, defaults to the pod IP."', args=[d.arg(name='host', type=d.T.string)]),
            withHost(host): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { tcpSocket+: { host: host } } } } } },
            '#withPort':: d.fn(help='', args=[d.arg(name='port', type=d.T.string)]),
            withPort(port): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { tcpSocket+: { port: port } } } } } },
          },
          '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
          withFailureThreshold(failureThreshold): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { failureThreshold: failureThreshold } } } } },
          '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
          withInitialDelaySeconds(initialDelaySeconds): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { initialDelaySeconds: initialDelaySeconds } } } } },
          '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
          withPeriodSeconds(periodSeconds): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { periodSeconds: periodSeconds } } } } },
          '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
          withSuccessThreshold(successThreshold): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { successThreshold: successThreshold } } } } },
          '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
          withTimeoutSeconds(timeoutSeconds): { workflowSpec+: { templateDefaults+: { script+: { startupProbe+: { timeoutSeconds: timeoutSeconds } } } } },
        },
        '#withArgs':: d.fn(help="\"Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"", args=[d.arg(name='args', type=d.T.array)]),
        withArgs(args): { workflowSpec+: { templateDefaults+: { script+: { args: if std.isArray(v=args) then args else [args] } } } },
        '#withArgsMixin':: d.fn(help="\"Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='args', type=d.T.array)]),
        withArgsMixin(args): { workflowSpec+: { templateDefaults+: { script+: { args+: if std.isArray(v=args) then args else [args] } } } },
        '#withCommand':: d.fn(help="\"Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"", args=[d.arg(name='command', type=d.T.array)]),
        withCommand(command): { workflowSpec+: { templateDefaults+: { script+: { command: if std.isArray(v=command) then command else [command] } } } },
        '#withCommandMixin':: d.fn(help="\"Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='command', type=d.T.array)]),
        withCommandMixin(command): { workflowSpec+: { templateDefaults+: { script+: { command+: if std.isArray(v=command) then command else [command] } } } },
        '#withEnv':: d.fn(help='"List of environment variables to set in the container. Cannot be updated."', args=[d.arg(name='env', type=d.T.array)]),
        withEnv(env): { workflowSpec+: { templateDefaults+: { script+: { env: if std.isArray(v=env) then env else [env] } } } },
        '#withEnvFrom':: d.fn(help='"List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated."', args=[d.arg(name='envFrom', type=d.T.array)]),
        withEnvFrom(envFrom): { workflowSpec+: { templateDefaults+: { script+: { envFrom: if std.isArray(v=envFrom) then envFrom else [envFrom] } } } },
        '#withEnvFromMixin':: d.fn(help='"List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='envFrom', type=d.T.array)]),
        withEnvFromMixin(envFrom): { workflowSpec+: { templateDefaults+: { script+: { envFrom+: if std.isArray(v=envFrom) then envFrom else [envFrom] } } } },
        '#withEnvMixin':: d.fn(help='"List of environment variables to set in the container. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='env', type=d.T.array)]),
        withEnvMixin(env): { workflowSpec+: { templateDefaults+: { script+: { env+: if std.isArray(v=env) then env else [env] } } } },
        '#withImage':: d.fn(help='"Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets."', args=[d.arg(name='image', type=d.T.string)]),
        withImage(image): { workflowSpec+: { templateDefaults+: { script+: { image: image } } } },
        '#withImagePullPolicy':: d.fn(help='"Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images"', args=[d.arg(name='imagePullPolicy', type=d.T.string)]),
        withImagePullPolicy(imagePullPolicy): { workflowSpec+: { templateDefaults+: { script+: { imagePullPolicy: imagePullPolicy } } } },
        '#withName':: d.fn(help='"Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { workflowSpec+: { templateDefaults+: { script+: { name: name } } } },
        '#withPorts':: d.fn(help='"List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \\"0.0.0.0\\" address inside a container will be accessible from the network. Cannot be updated."', args=[d.arg(name='ports', type=d.T.array)]),
        withPorts(ports): { workflowSpec+: { templateDefaults+: { script+: { ports: if std.isArray(v=ports) then ports else [ports] } } } },
        '#withPortsMixin':: d.fn(help='"List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \\"0.0.0.0\\" address inside a container will be accessible from the network. Cannot be updated."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ports', type=d.T.array)]),
        withPortsMixin(ports): { workflowSpec+: { templateDefaults+: { script+: { ports+: if std.isArray(v=ports) then ports else [ports] } } } },
        '#withSource':: d.fn(help='"Source contains the source code of the script to execute"', args=[d.arg(name='source', type=d.T.string)]),
        withSource(source): { workflowSpec+: { templateDefaults+: { script+: { source: source } } } },
        '#withStdin':: d.fn(help='"Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false."', args=[d.arg(name='stdin', type=d.T.boolean)]),
        withStdin(stdin): { workflowSpec+: { templateDefaults+: { script+: { stdin: stdin } } } },
        '#withStdinOnce':: d.fn(help='"Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false"', args=[d.arg(name='stdinOnce', type=d.T.boolean)]),
        withStdinOnce(stdinOnce): { workflowSpec+: { templateDefaults+: { script+: { stdinOnce: stdinOnce } } } },
        '#withTerminationMessagePath':: d.fn(help="\"Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated.\"", args=[d.arg(name='terminationMessagePath', type=d.T.string)]),
        withTerminationMessagePath(terminationMessagePath): { workflowSpec+: { templateDefaults+: { script+: { terminationMessagePath: terminationMessagePath } } } },
        '#withTerminationMessagePolicy':: d.fn(help='"Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated."', args=[d.arg(name='terminationMessagePolicy', type=d.T.string)]),
        withTerminationMessagePolicy(terminationMessagePolicy): { workflowSpec+: { templateDefaults+: { script+: { terminationMessagePolicy: terminationMessagePolicy } } } },
        '#withTty':: d.fn(help="\"Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false.\"", args=[d.arg(name='tty', type=d.T.boolean)]),
        withTty(tty): { workflowSpec+: { templateDefaults+: { script+: { tty: tty } } } },
        '#withVolumeDevices':: d.fn(help='"volumeDevices is the list of block devices to be used by the container."', args=[d.arg(name='volumeDevices', type=d.T.array)]),
        withVolumeDevices(volumeDevices): { workflowSpec+: { templateDefaults+: { script+: { volumeDevices: if std.isArray(v=volumeDevices) then volumeDevices else [volumeDevices] } } } },
        '#withVolumeDevicesMixin':: d.fn(help='"volumeDevices is the list of block devices to be used by the container."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumeDevices', type=d.T.array)]),
        withVolumeDevicesMixin(volumeDevices): { workflowSpec+: { templateDefaults+: { script+: { volumeDevices+: if std.isArray(v=volumeDevices) then volumeDevices else [volumeDevices] } } } },
        '#withVolumeMounts':: d.fn(help="\"Pod volumes to mount into the container's filesystem. Cannot be updated.\"", args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMounts(volumeMounts): { workflowSpec+: { templateDefaults+: { script+: { volumeMounts: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
        '#withVolumeMountsMixin':: d.fn(help="\"Pod volumes to mount into the container's filesystem. Cannot be updated.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='volumeMounts', type=d.T.array)]),
        withVolumeMountsMixin(volumeMounts): { workflowSpec+: { templateDefaults+: { script+: { volumeMounts+: if std.isArray(v=volumeMounts) then volumeMounts else [volumeMounts] } } } },
        '#withWorkingDir':: d.fn(help="\"Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated.\"", args=[d.arg(name='workingDir', type=d.T.string)]),
        withWorkingDir(workingDir): { workflowSpec+: { templateDefaults+: { script+: { workingDir: workingDir } } } },
      },
      '#securityContext':: d.obj(help='"PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext."'),
      securityContext: {
        '#seLinuxOptions':: d.obj(help='"SELinuxOptions are the labels to be applied to the container"'),
        seLinuxOptions: {
          '#withLevel':: d.fn(help='"Level is SELinux level label that applies to the container."', args=[d.arg(name='level', type=d.T.string)]),
          withLevel(level): { workflowSpec+: { templateDefaults+: { securityContext+: { seLinuxOptions+: { level: level } } } } },
          '#withRole':: d.fn(help='"Role is a SELinux role label that applies to the container."', args=[d.arg(name='role', type=d.T.string)]),
          withRole(role): { workflowSpec+: { templateDefaults+: { securityContext+: { seLinuxOptions+: { role: role } } } } },
          '#withType':: d.fn(help='"Type is a SELinux type label that applies to the container."', args=[d.arg(name='type', type=d.T.string)]),
          withType(type): { workflowSpec+: { templateDefaults+: { securityContext+: { seLinuxOptions+: { type: type } } } } },
          '#withUser':: d.fn(help='"User is a SELinux user label that applies to the container."', args=[d.arg(name='user', type=d.T.string)]),
          withUser(user): { workflowSpec+: { templateDefaults+: { securityContext+: { seLinuxOptions+: { user: user } } } } },
        },
        '#windowsOptions':: d.obj(help='"WindowsSecurityContextOptions contain Windows-specific options and credentials."'),
        windowsOptions: {
          '#withGmsaCredentialSpec':: d.fn(help='"GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpec', type=d.T.string)]),
          withGmsaCredentialSpec(gmsaCredentialSpec): { workflowSpec+: { templateDefaults+: { securityContext+: { windowsOptions+: { gmsaCredentialSpec: gmsaCredentialSpec } } } } },
          '#withGmsaCredentialSpecName':: d.fn(help='"GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag."', args=[d.arg(name='gmsaCredentialSpecName', type=d.T.string)]),
          withGmsaCredentialSpecName(gmsaCredentialSpecName): { workflowSpec+: { templateDefaults+: { securityContext+: { windowsOptions+: { gmsaCredentialSpecName: gmsaCredentialSpecName } } } } },
          '#withRunAsUserName':: d.fn(help='"The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. This field is beta-level and may be disabled with the WindowsRunAsUserName feature flag."', args=[d.arg(name='runAsUserName', type=d.T.string)]),
          withRunAsUserName(runAsUserName): { workflowSpec+: { templateDefaults+: { securityContext+: { windowsOptions+: { runAsUserName: runAsUserName } } } } },
        },
        '#withFsGroup':: d.fn(help="\"A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod:\\n\\n1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw----\\n\\nIf unset, the Kubelet will not modify the ownership and permissions of any volume.\"", args=[d.arg(name='fsGroup', type=d.T.integer)]),
        withFsGroup(fsGroup): { workflowSpec+: { templateDefaults+: { securityContext+: { fsGroup: fsGroup } } } },
        '#withRunAsGroup':: d.fn(help='"The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container."', args=[d.arg(name='runAsGroup', type=d.T.integer)]),
        withRunAsGroup(runAsGroup): { workflowSpec+: { templateDefaults+: { securityContext+: { runAsGroup: runAsGroup } } } },
        '#withRunAsNonRoot':: d.fn(help='"Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."', args=[d.arg(name='runAsNonRoot', type=d.T.boolean)]),
        withRunAsNonRoot(runAsNonRoot): { workflowSpec+: { templateDefaults+: { securityContext+: { runAsNonRoot: runAsNonRoot } } } },
        '#withRunAsUser':: d.fn(help='"The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container."', args=[d.arg(name='runAsUser', type=d.T.integer)]),
        withRunAsUser(runAsUser): { workflowSpec+: { templateDefaults+: { securityContext+: { runAsUser: runAsUser } } } },
        '#withSupplementalGroups':: d.fn(help="\"A list of groups applied to the first process run in each container, in addition to the container's primary GID.  If unspecified, no groups will be added to any container.\"", args=[d.arg(name='supplementalGroups', type=d.T.array)]),
        withSupplementalGroups(supplementalGroups): { workflowSpec+: { templateDefaults+: { securityContext+: { supplementalGroups: if std.isArray(v=supplementalGroups) then supplementalGroups else [supplementalGroups] } } } },
        '#withSupplementalGroupsMixin':: d.fn(help="\"A list of groups applied to the first process run in each container, in addition to the container's primary GID.  If unspecified, no groups will be added to any container.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='supplementalGroups', type=d.T.array)]),
        withSupplementalGroupsMixin(supplementalGroups): { workflowSpec+: { templateDefaults+: { securityContext+: { supplementalGroups+: if std.isArray(v=supplementalGroups) then supplementalGroups else [supplementalGroups] } } } },
        '#withSysctls':: d.fn(help='"Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch."', args=[d.arg(name='sysctls', type=d.T.array)]),
        withSysctls(sysctls): { workflowSpec+: { templateDefaults+: { securityContext+: { sysctls: if std.isArray(v=sysctls) then sysctls else [sysctls] } } } },
        '#withSysctlsMixin':: d.fn(help='"Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sysctls', type=d.T.array)]),
        withSysctlsMixin(sysctls): { workflowSpec+: { templateDefaults+: { securityContext+: { sysctls+: if std.isArray(v=sysctls) then sysctls else [sysctls] } } } },
      },
      '#suspend':: d.obj(help='"SuspendTemplate is a template subtype to suspend a workflow at a predetermined point in time"'),
      suspend: {
        '#withDuration':: d.fn(help='"Duration is the seconds to wait before automatically resuming a template"', args=[d.arg(name='duration', type=d.T.string)]),
        withDuration(duration): { workflowSpec+: { templateDefaults+: { suspend+: { duration: duration } } } },
      },
      '#synchronization':: d.obj(help='"Synchronization holds synchronization lock configuration"'),
      synchronization: {
        '#mutex':: d.obj(help='"Mutex holds Mutex configuration"'),
        mutex: {
          '#withName':: d.fn(help='"name of the mutex"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { workflowSpec+: { templateDefaults+: { synchronization+: { mutex+: { name: name } } } } },
        },
        '#semaphore':: d.obj(help='"SemaphoreRef is a reference of Semaphore"'),
        semaphore: {
          '#configMapKeyRef':: d.obj(help='"Selects a key from a ConfigMap."'),
          configMapKeyRef: {
            '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { workflowSpec+: { templateDefaults+: { synchronization+: { semaphore+: { configMapKeyRef+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { workflowSpec+: { templateDefaults+: { synchronization+: { semaphore+: { configMapKeyRef+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { workflowSpec+: { templateDefaults+: { synchronization+: { semaphore+: { configMapKeyRef+: { optional: optional } } } } } },
          },
        },
      },
      '#withActiveDeadlineSeconds':: d.fn(help='', args=[d.arg(name='activeDeadlineSeconds', type=d.T.string)]),
      withActiveDeadlineSeconds(activeDeadlineSeconds): { workflowSpec+: { templateDefaults+: { activeDeadlineSeconds: activeDeadlineSeconds } } },
      '#withAutomountServiceAccountToken':: d.fn(help='"AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."', args=[d.arg(name='automountServiceAccountToken', type=d.T.boolean)]),
      withAutomountServiceAccountToken(automountServiceAccountToken): { workflowSpec+: { templateDefaults+: { automountServiceAccountToken: automountServiceAccountToken } } },
      '#withDaemon':: d.fn(help='"Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness"', args=[d.arg(name='daemon', type=d.T.boolean)]),
      withDaemon(daemon): { workflowSpec+: { templateDefaults+: { daemon: daemon } } },
      '#withFailFast':: d.fn(help='"FailFast, if specified, will fail this template if any of its child pods has failed. This is useful for when this template is expanded with `withItems`, etc."', args=[d.arg(name='failFast', type=d.T.boolean)]),
      withFailFast(failFast): { workflowSpec+: { templateDefaults+: { failFast: failFast } } },
      '#withHostAliases':: d.fn(help='"HostAliases is an optional list of hosts and IPs that will be injected into the pod spec"', args=[d.arg(name='hostAliases', type=d.T.array)]),
      withHostAliases(hostAliases): { workflowSpec+: { templateDefaults+: { hostAliases: if std.isArray(v=hostAliases) then hostAliases else [hostAliases] } } },
      '#withHostAliasesMixin':: d.fn(help='"HostAliases is an optional list of hosts and IPs that will be injected into the pod spec"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='hostAliases', type=d.T.array)]),
      withHostAliasesMixin(hostAliases): { workflowSpec+: { templateDefaults+: { hostAliases+: if std.isArray(v=hostAliases) then hostAliases else [hostAliases] } } },
      '#withInitContainers':: d.fn(help='"InitContainers is a list of containers which run before the main container."', args=[d.arg(name='initContainers', type=d.T.array)]),
      withInitContainers(initContainers): { workflowSpec+: { templateDefaults+: { initContainers: if std.isArray(v=initContainers) then initContainers else [initContainers] } } },
      '#withInitContainersMixin':: d.fn(help='"InitContainers is a list of containers which run before the main container."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='initContainers', type=d.T.array)]),
      withInitContainersMixin(initContainers): { workflowSpec+: { templateDefaults+: { initContainers+: if std.isArray(v=initContainers) then initContainers else [initContainers] } } },
      '#withName':: d.fn(help='"Name is the name of the template"', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { workflowSpec+: { templateDefaults+: { name: name } } },
      '#withNodeSelector':: d.fn(help='"NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level."', args=[d.arg(name='nodeSelector', type=d.T.object)]),
      withNodeSelector(nodeSelector): { workflowSpec+: { templateDefaults+: { nodeSelector: nodeSelector } } },
      '#withNodeSelectorMixin':: d.fn(help='"NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelector', type=d.T.object)]),
      withNodeSelectorMixin(nodeSelector): { workflowSpec+: { templateDefaults+: { nodeSelector+: nodeSelector } } },
      '#withParallelism':: d.fn(help='"Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total."', args=[d.arg(name='parallelism', type=d.T.integer)]),
      withParallelism(parallelism): { workflowSpec+: { templateDefaults+: { parallelism: parallelism } } },
      '#withPodSpecPatch':: d.fn(help='"PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits)."', args=[d.arg(name='podSpecPatch', type=d.T.string)]),
      withPodSpecPatch(podSpecPatch): { workflowSpec+: { templateDefaults+: { podSpecPatch: podSpecPatch } } },
      '#withPriority':: d.fn(help='"Priority to apply to workflow pods."', args=[d.arg(name='priority', type=d.T.integer)]),
      withPriority(priority): { workflowSpec+: { templateDefaults+: { priority: priority } } },
      '#withPriorityClassName':: d.fn(help='"PriorityClassName to apply to workflow pods."', args=[d.arg(name='priorityClassName', type=d.T.string)]),
      withPriorityClassName(priorityClassName): { workflowSpec+: { templateDefaults+: { priorityClassName: priorityClassName } } },
      '#withSchedulerName':: d.fn(help='"If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler."', args=[d.arg(name='schedulerName', type=d.T.string)]),
      withSchedulerName(schedulerName): { workflowSpec+: { templateDefaults+: { schedulerName: schedulerName } } },
      '#withServiceAccountName':: d.fn(help='"ServiceAccountName to apply to workflow pods"', args=[d.arg(name='serviceAccountName', type=d.T.string)]),
      withServiceAccountName(serviceAccountName): { workflowSpec+: { templateDefaults+: { serviceAccountName: serviceAccountName } } },
      '#withSidecars':: d.fn(help='"Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes"', args=[d.arg(name='sidecars', type=d.T.array)]),
      withSidecars(sidecars): { workflowSpec+: { templateDefaults+: { sidecars: if std.isArray(v=sidecars) then sidecars else [sidecars] } } },
      '#withSidecarsMixin':: d.fn(help='"Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sidecars', type=d.T.array)]),
      withSidecarsMixin(sidecars): { workflowSpec+: { templateDefaults+: { sidecars+: if std.isArray(v=sidecars) then sidecars else [sidecars] } } },
      '#withSteps':: d.fn(help='"Steps define a series of sequential/parallel workflow steps"', args=[d.arg(name='steps', type=d.T.array)]),
      withSteps(steps): { workflowSpec+: { templateDefaults+: { steps: if std.isArray(v=steps) then steps else [steps] } } },
      '#withStepsMixin':: d.fn(help='"Steps define a series of sequential/parallel workflow steps"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='steps', type=d.T.array)]),
      withStepsMixin(steps): { workflowSpec+: { templateDefaults+: { steps+: if std.isArray(v=steps) then steps else [steps] } } },
      '#withTimeout':: d.fn(help="\"Timout allows to set the total node execution timeout duration counting from the node's start time. This duration also includes time in which the node spends in Pending state. This duration may not be applied to Step or DAG templates.\"", args=[d.arg(name='timeout', type=d.T.string)]),
      withTimeout(timeout): { workflowSpec+: { templateDefaults+: { timeout: timeout } } },
      '#withTolerations':: d.fn(help='"Tolerations to apply to workflow pods."', args=[d.arg(name='tolerations', type=d.T.array)]),
      withTolerations(tolerations): { workflowSpec+: { templateDefaults+: { tolerations: if std.isArray(v=tolerations) then tolerations else [tolerations] } } },
      '#withTolerationsMixin':: d.fn(help='"Tolerations to apply to workflow pods."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tolerations', type=d.T.array)]),
      withTolerationsMixin(tolerations): { workflowSpec+: { templateDefaults+: { tolerations+: if std.isArray(v=tolerations) then tolerations else [tolerations] } } },
      '#withVolumes':: d.fn(help='"Volumes is a list of volumes that can be mounted by containers in a template."', args=[d.arg(name='volumes', type=d.T.array)]),
      withVolumes(volumes): { workflowSpec+: { templateDefaults+: { volumes: if std.isArray(v=volumes) then volumes else [volumes] } } },
      '#withVolumesMixin':: d.fn(help='"Volumes is a list of volumes that can be mounted by containers in a template."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumes', type=d.T.array)]),
      withVolumesMixin(volumes): { workflowSpec+: { templateDefaults+: { volumes+: if std.isArray(v=volumes) then volumes else [volumes] } } },
    },
    '#ttlStrategy':: d.obj(help='"TTLStrategy is the strategy for the time to live depending on if the workflow succeeded or failed"'),
    ttlStrategy: {
      '#withSecondsAfterCompletion':: d.fn(help='"SecondsAfterCompletion is the number of seconds to live after completion"', args=[d.arg(name='secondsAfterCompletion', type=d.T.integer)]),
      withSecondsAfterCompletion(secondsAfterCompletion): { workflowSpec+: { ttlStrategy+: { secondsAfterCompletion: secondsAfterCompletion } } },
      '#withSecondsAfterFailure':: d.fn(help='"SecondsAfterFailure is the number of seconds to live after failure"', args=[d.arg(name='secondsAfterFailure', type=d.T.integer)]),
      withSecondsAfterFailure(secondsAfterFailure): { workflowSpec+: { ttlStrategy+: { secondsAfterFailure: secondsAfterFailure } } },
      '#withSecondsAfterSuccess':: d.fn(help='"SecondsAfterSuccess is the number of seconds to live after success"', args=[d.arg(name='secondsAfterSuccess', type=d.T.integer)]),
      withSecondsAfterSuccess(secondsAfterSuccess): { workflowSpec+: { ttlStrategy+: { secondsAfterSuccess: secondsAfterSuccess } } },
    },
    '#volumeClaimGC':: d.obj(help='"VolumeClaimGC describes how to delete volumes from completed Workflows"'),
    volumeClaimGC: {
      '#withStrategy':: d.fn(help='"Strategy is the strategy to use. One of \\"OnWorkflowCompletion\\", \\"OnWorkflowSuccess\\', args=[d.arg(name='strategy', type=d.T.string)]),
      withStrategy(strategy): { workflowSpec+: { volumeClaimGC+: { strategy: strategy } } },
    },
    '#withActiveDeadlineSeconds':: d.fn(help='"Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow"', args=[d.arg(name='activeDeadlineSeconds', type=d.T.integer)]),
    withActiveDeadlineSeconds(activeDeadlineSeconds): { workflowSpec+: { activeDeadlineSeconds: activeDeadlineSeconds } },
    '#withArchiveLogs':: d.fn(help='"ArchiveLogs indicates if the container logs should be archived"', args=[d.arg(name='archiveLogs', type=d.T.boolean)]),
    withArchiveLogs(archiveLogs): { workflowSpec+: { archiveLogs: archiveLogs } },
    '#withAutomountServiceAccountToken':: d.fn(help='"AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."', args=[d.arg(name='automountServiceAccountToken', type=d.T.boolean)]),
    withAutomountServiceAccountToken(automountServiceAccountToken): { workflowSpec+: { automountServiceAccountToken: automountServiceAccountToken } },
    '#withDnsPolicy':: d.fn(help="\"Set DNS policy for the pod. Defaults to \\\"ClusterFirst\\\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'.\"", args=[d.arg(name='dnsPolicy', type=d.T.string)]),
    withDnsPolicy(dnsPolicy): { workflowSpec+: { dnsPolicy: dnsPolicy } },
    '#withEntrypoint':: d.fn(help='"Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1."', args=[d.arg(name='entrypoint', type=d.T.string)]),
    withEntrypoint(entrypoint): { workflowSpec+: { entrypoint: entrypoint } },
    '#withHostAliases':: d.fn(help='', args=[d.arg(name='hostAliases', type=d.T.array)]),
    withHostAliases(hostAliases): { workflowSpec+: { hostAliases: if std.isArray(v=hostAliases) then hostAliases else [hostAliases] } },
    '#withHostAliasesMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='hostAliases', type=d.T.array)]),
    withHostAliasesMixin(hostAliases): { workflowSpec+: { hostAliases+: if std.isArray(v=hostAliases) then hostAliases else [hostAliases] } },
    '#withHostNetwork':: d.fn(help='"Host networking requested for this workflow pod. Default to false."', args=[d.arg(name='hostNetwork', type=d.T.boolean)]),
    withHostNetwork(hostNetwork): { workflowSpec+: { hostNetwork: hostNetwork } },
    '#withImagePullSecrets':: d.fn(help='"ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"', args=[d.arg(name='imagePullSecrets', type=d.T.array)]),
    withImagePullSecrets(imagePullSecrets): { workflowSpec+: { imagePullSecrets: if std.isArray(v=imagePullSecrets) then imagePullSecrets else [imagePullSecrets] } },
    '#withImagePullSecretsMixin':: d.fn(help='"ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='imagePullSecrets', type=d.T.array)]),
    withImagePullSecretsMixin(imagePullSecrets): { workflowSpec+: { imagePullSecrets+: if std.isArray(v=imagePullSecrets) then imagePullSecrets else [imagePullSecrets] } },
    '#withNodeSelector':: d.fn(help='"NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template."', args=[d.arg(name='nodeSelector', type=d.T.object)]),
    withNodeSelector(nodeSelector): { workflowSpec+: { nodeSelector: nodeSelector } },
    '#withNodeSelectorMixin':: d.fn(help='"NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelector', type=d.T.object)]),
    withNodeSelectorMixin(nodeSelector): { workflowSpec+: { nodeSelector+: nodeSelector } },
    '#withOnExit':: d.fn(help='"OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1."', args=[d.arg(name='onExit', type=d.T.string)]),
    withOnExit(onExit): { workflowSpec+: { onExit: onExit } },
    '#withParallelism':: d.fn(help='"Parallelism limits the max total parallel pods that can execute at the same time in a workflow"', args=[d.arg(name='parallelism', type=d.T.integer)]),
    withParallelism(parallelism): { workflowSpec+: { parallelism: parallelism } },
    '#withPodPriority':: d.fn(help='"Priority to apply to workflow pods."', args=[d.arg(name='podPriority', type=d.T.integer)]),
    withPodPriority(podPriority): { workflowSpec+: { podPriority: podPriority } },
    '#withPodPriorityClassName':: d.fn(help='"PriorityClassName to apply to workflow pods."', args=[d.arg(name='podPriorityClassName', type=d.T.string)]),
    withPodPriorityClassName(podPriorityClassName): { workflowSpec+: { podPriorityClassName: podPriorityClassName } },
    '#withPodSpecPatch':: d.fn(help='"PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits)."', args=[d.arg(name='podSpecPatch', type=d.T.string)]),
    withPodSpecPatch(podSpecPatch): { workflowSpec+: { podSpecPatch: podSpecPatch } },
    '#withPriority':: d.fn(help='"Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first."', args=[d.arg(name='priority', type=d.T.integer)]),
    withPriority(priority): { workflowSpec+: { priority: priority } },
    '#withSchedulerName':: d.fn(help="\"Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified.\"", args=[d.arg(name='schedulerName', type=d.T.string)]),
    withSchedulerName(schedulerName): { workflowSpec+: { schedulerName: schedulerName } },
    '#withServiceAccountName':: d.fn(help='"ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as."', args=[d.arg(name='serviceAccountName', type=d.T.string)]),
    withServiceAccountName(serviceAccountName): { workflowSpec+: { serviceAccountName: serviceAccountName } },
    '#withShutdown':: d.fn(help='"Shutdown will shutdown the workflow according to its ShutdownStrategy"', args=[d.arg(name='shutdown', type=d.T.string)]),
    withShutdown(shutdown): { workflowSpec+: { shutdown: shutdown } },
    '#withSuspend':: d.fn(help='"Suspend will suspend the workflow and prevent execution of any future steps in the workflow"', args=[d.arg(name='suspend', type=d.T.boolean)]),
    withSuspend(suspend): { workflowSpec+: { suspend: suspend } },
    '#withTemplates':: d.fn(help='"Templates is a list of workflow templates used in a workflow"', args=[d.arg(name='templates', type=d.T.array)]),
    withTemplates(templates): { workflowSpec+: { templates: if std.isArray(v=templates) then templates else [templates] } },
    '#withTemplatesMixin':: d.fn(help='"Templates is a list of workflow templates used in a workflow"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='templates', type=d.T.array)]),
    withTemplatesMixin(templates): { workflowSpec+: { templates+: if std.isArray(v=templates) then templates else [templates] } },
    '#withTolerations':: d.fn(help='"Tolerations to apply to workflow pods."', args=[d.arg(name='tolerations', type=d.T.array)]),
    withTolerations(tolerations): { workflowSpec+: { tolerations: if std.isArray(v=tolerations) then tolerations else [tolerations] } },
    '#withTolerationsMixin':: d.fn(help='"Tolerations to apply to workflow pods."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tolerations', type=d.T.array)]),
    withTolerationsMixin(tolerations): { workflowSpec+: { tolerations+: if std.isArray(v=tolerations) then tolerations else [tolerations] } },
    '#withVolumeClaimTemplates':: d.fn(help='"VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow"', args=[d.arg(name='volumeClaimTemplates', type=d.T.array)]),
    withVolumeClaimTemplates(volumeClaimTemplates): { workflowSpec+: { volumeClaimTemplates: if std.isArray(v=volumeClaimTemplates) then volumeClaimTemplates else [volumeClaimTemplates] } },
    '#withVolumeClaimTemplatesMixin':: d.fn(help='"VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumeClaimTemplates', type=d.T.array)]),
    withVolumeClaimTemplatesMixin(volumeClaimTemplates): { workflowSpec+: { volumeClaimTemplates+: if std.isArray(v=volumeClaimTemplates) then volumeClaimTemplates else [volumeClaimTemplates] } },
    '#withVolumes':: d.fn(help='"Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1."', args=[d.arg(name='volumes', type=d.T.array)]),
    withVolumes(volumes): { workflowSpec+: { volumes: if std.isArray(v=volumes) then volumes else [volumes] } },
    '#withVolumesMixin':: d.fn(help='"Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='volumes', type=d.T.array)]),
    withVolumesMixin(volumes): { workflowSpec+: { volumes+: if std.isArray(v=volumes) then volumes else [volumes] } },
    '#workflowTemplateRef':: d.obj(help='"WorkflowTemplateRef is a reference to a WorkflowTemplate resource."'),
    workflowTemplateRef: {
      '#withClusterScope':: d.fn(help='"ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate)."', args=[d.arg(name='clusterScope', type=d.T.boolean)]),
      withClusterScope(clusterScope): { workflowSpec+: { workflowTemplateRef+: { clusterScope: clusterScope } } },
      '#withName':: d.fn(help='"Name is the resource name of the workflow template."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { workflowSpec+: { workflowTemplateRef+: { name: name } } },
    },
  },
  '#mixin': 'ignore',
  mixin: self,
}
